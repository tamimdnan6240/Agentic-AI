# -*- coding: utf-8 -*-
"""01-05-2026-BM25+Knowledge-graph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hvMkdXNrw7h_ysEo19oRb5xfnMdS6mrr
"""

# =========================
# 0) Install (RELIABLE)
# =========================
!pip -q uninstall -y autogen pyautogen ag2 >/dev/null 2>&1
!pip -q install -U "ag2[openai]" python-dotenv

!pip show ag2
!python -c "import autogen; print('autogen version:', getattr(autogen,'__version__', 'unknown'))"

# ============================================================
# 1) Env (DO NOT hardcode keys)
#   Colab: Runtime > Secrets > add OPENAI_API_KEY
# ============================================================
import os, json, time, re, hashlib, sqlite3, sys
from dotenv import load_dotenv
load_dotenv()

# Read from Colab Secrets / .env
os.environ["OPENAI_API_KEY"] =   # set via Secrets
os.environ["OPENAI_MODEL_NAME"] = os.environ.get("OPENAI_MODEL_NAME", "gpt-4o-mini")

print("OPENAI_MODEL_NAME:", os.environ["OPENAI_MODEL_NAME"])
print("OPENAI_API_KEY present?", bool(os.getenv("OPENAI_API_KEY")))

# ============================================================
# 2) Mount Drive + File Path(s)
# ============================================================
from google.colab import drive

FILE_PATH = "/content/drive/MyDrive/Network Maintenace - With & without maintenace/Original-data/2017-2019 (no main).csv"
csv_paths = [FILE_PATH]

if not os.path.exists("/content/drive/MyDrive"):
    drive.mount("/content/drive", force_remount=True)

for p in csv_paths:
    if not os.path.exists(p):
        raise FileNotFoundError(f"CSV path not found:\n{p}")

print("Mounted?", os.path.exists("/content/drive/MyDrive"))
print("CSV paths exist?", all(os.path.exists(p) for p in csv_paths))

# ============================================================
# 3) Workspace + Hybrid Persistent Memory (Notebook version)
#   Then EXPORT to mm_runtime.py (AutoGen-safe)
# ============================================================
from typing import Optional, List, Dict, Any, Tuple
import numpy as np
import pandas as pd
import networkx as nx

WORK_DIR = "/content/pavement_agentic_workspace"
os.makedirs(WORK_DIR, exist_ok=True)

MEM_PATH      = os.path.join(WORK_DIR, "memory.jsonl")
DB_PATH       = os.path.join(WORK_DIR, "memory_hybrid.sqlite")
VEC_PATH      = os.path.join(WORK_DIR, "memory_vectors.npz")
KG_PATH       = os.path.join(WORK_DIR, "knowledge_graph.graphml")
REGISTRY_PATH = os.path.join(WORK_DIR, "file_path_registry.json")

# -------------------------
# JSONL audit
# -------------------------
def _mem_load_raw() -> List[Dict[str, Any]]:
    if not os.path.exists(MEM_PATH):
        return []
    rows = []
    with open(MEM_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rows.append(json.loads(line))
            except Exception:
                pass
    return rows

def mem_add_jsonl(kind: str, text: str, meta: Optional[dict] = None) -> str:
    meta = meta or {}
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    raw = f"{ts}|{kind}|{text}|{json.dumps(meta, sort_keys=True)}".encode("utf-8")
    memory_id = hashlib.sha256(raw).hexdigest()[:16]
    rec = {"id": memory_id, "ts": ts, "kind": str(kind), "text": str(text).strip(), "meta": meta}
    os.makedirs(WORK_DIR, exist_ok=True)
    with open(MEM_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    return memory_id

# -------------------------
# SQLite hybrid store
# -------------------------
def _db_connect() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH)
    conn.execute("PRAGMA journal_mode=WAL;")

    conn.execute(
        "CREATE TABLE IF NOT EXISTS memory_struct ("
        "id TEXT PRIMARY KEY, ts TEXT, kind TEXT, text TEXT, meta_json TEXT"
        ");"
    )
    conn.execute(
        "CREATE VIRTUAL TABLE IF NOT EXISTS memory_fts USING fts5("
        "id UNINDEXED, kind, text, content='', tokenize='porter'"
        ");"
    )
    conn.execute(
        "CREATE TABLE IF NOT EXISTS doc_summaries ("
        "doc_id TEXT PRIMARY KEY, title TEXT, file_path TEXT, summary TEXT, meta_json TEXT"
        ");"
    )
    conn.execute(
        "CREATE TABLE IF NOT EXISTS nodes ("
        "node_id TEXT PRIMARY KEY, node_type TEXT, title TEXT, text TEXT, doc_id TEXT, meta_json TEXT"
        ");"
    )
    conn.execute(
        "CREATE TABLE IF NOT EXISTS edges ("
        "src TEXT, rel TEXT, dst TEXT, meta_json TEXT, PRIMARY KEY (src, rel, dst)"
        ");"
    )

    conn.commit()
    return conn

def keyword_index_add(memory_id: str, ts: str, kind: str, text: str, meta: dict):
    conn = _db_connect()
    conn.execute(
        "INSERT OR REPLACE INTO memory_struct(id, ts, kind, text, meta_json) VALUES (?, ?, ?, ?, ?)",
        (memory_id, ts, kind, text, json.dumps(meta, ensure_ascii=False))
    )
    conn.execute("INSERT INTO memory_fts(id, kind, text) VALUES (?, ?, ?)", (memory_id, kind, text))
    conn.commit()
    conn.close()

def keyword_search(query: str, k: int = 5, kind: Optional[str] = None) -> List[Dict[str, Any]]:
    conn = _db_connect()
    where_kind = "AND kind = ?" if kind else ""
    params = [query]
    if kind:
        params.append(kind)
    params.append(k)

    sql = (
        "SELECT id, kind, text, bm25(memory_fts) AS score "
        "FROM memory_fts "
        "WHERE memory_fts MATCH ? "
        f"{where_kind} "
        "ORDER BY score "
        "LIMIT ?;"
    )
    rows = conn.execute(sql, params).fetchall()
    conn.close()

    out = []
    for rid, rkind, rtext, score in rows:
        out.append({"id": rid, "kind": rkind, "text": rtext, "score": float(-score)})
    return out

# -------------------------
# Semantic vectors (NPZ)
# -------------------------
def _load_vec_store() -> Tuple[List[str], np.ndarray]:
    if not os.path.exists(VEC_PATH):
        return [], np.zeros((0, 0), dtype=np.float32)
    data = np.load(VEC_PATH, allow_pickle=True)
    return data["ids"].tolist(), data["vecs"].astype(np.float32)

def _save_vec_store(ids: List[str], vecs: np.ndarray):
    np.savez(VEC_PATH, ids=np.array(ids, dtype=object), vecs=vecs.astype(np.float32))

def _normalize_rows(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return X
    norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12
    return X / norms

def embed_text(text: str) -> np.ndarray:
    try:
        from sentence_transformers import SentenceTransformer
        global _ST_MODEL
        if "_ST_MODEL" not in globals():
            _ST_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
        v = _ST_MODEL.encode([text], normalize_embeddings=True)[0]
        return np.array(v, dtype=np.float32)
    except Exception:
        dim = 512
        v = np.zeros(dim, dtype=np.float32)
        for token in text.lower().split():
            h = int(hashlib.sha256(token.encode()).hexdigest(), 16)
            v[h % dim] += 1.0
        v = v / (np.linalg.norm(v) + 1e-12)
        return v

def semantic_add(memory_id: str, text: str):
    ids, vecs = _load_vec_store()
    v = embed_text(text).reshape(1, -1)

    if vecs.size == 0:
        ids = [memory_id]
        vecs = v
    else:
        if v.shape[1] != vecs.shape[1]:
            raise ValueError(f"Embedding dim mismatch: got {v.shape[1]} vs store {vecs.shape[1]}")
        ids.append(memory_id)
        vecs = np.vstack([vecs, v])

    _save_vec_store(ids, vecs)

def semantic_search(query: str, k: int = 5, kind: Optional[str] = None) -> List[Dict[str, Any]]:
    ids, vecs = _load_vec_store()
    if vecs.size == 0:
        return []

    q = embed_text(query).reshape(1, -1).astype(np.float32)
    vecs_n = _normalize_rows(vecs)
    q_n = _normalize_rows(q)
    sims = (vecs_n @ q_n.T).ravel()
    top = np.argsort(-sims)[: min(k * 5, len(ids))]

    # kind filter using memory_struct (optional)
    id2kind = None
    if kind and len(top) > 0:
        conn = _db_connect()
        ph = ",".join(["?"] * len(top))
        rows = conn.execute(
            f"SELECT id, kind FROM memory_struct WHERE id IN ({ph})",
            [ids[i] for i in top]
        ).fetchall()
        conn.close()
        id2kind = {rid: rk for rid, rk in rows}

    out = []
    for i in top:
        rid = ids[i]
        if kind and id2kind and (id2kind.get(rid) != kind):
            continue
        out.append({"id": rid, "score": float(sims[i])})
        if len(out) >= k:
            break
    return out

# -------------------------
# KG (NetworkX)
# -------------------------
def _kg_load() -> nx.MultiDiGraph:
    if os.path.exists(KG_PATH):
        try:
            g = nx.read_graphml(KG_PATH)
            mg = nx.MultiDiGraph()
            mg.add_nodes_from(g.nodes(data=True))
            mg.add_edges_from(g.edges(data=True))
            return mg
        except Exception:
            pass
    return nx.MultiDiGraph()

_KG = _kg_load()

def _kg_save():
    g = nx.DiGraph()
    g.add_nodes_from(_KG.nodes(data=True))
    for u, v, data in _KG.edges(data=True):
        g.add_edge(u, v, **data)
    nx.write_graphml(g, KG_PATH)

def kg_add_fact(subj: str, pred: str, obj: str, confidence: float = 1.0, meta: Optional[dict] = None, index_to_memory: bool = True):
    meta = meta or {}
    subj = str(subj); pred = str(pred); obj = str(obj)

    _KG.add_node(subj)
    _KG.add_node(obj)
    _KG.add_edge(subj, obj, relation=pred, confidence=float(confidence), **meta)
    _kg_save()

    if index_to_memory:
        mem_add_jsonl("kg_fact", f"KG FACT: ({subj}) -[{pred}]-> ({obj}) conf={confidence}",
                      {"subj": subj, "pred": pred, "obj": obj, "confidence": float(confidence), **meta})

# -------------------------
# Registry + artifacts
# -------------------------
def _stable_dataset_id(file_path: str) -> str:
    st = os.stat(file_path)
    raw = f"{file_path}|{st.st_size}|{int(st.st_mtime)}".encode("utf-8")
    return hashlib.sha256(raw).hexdigest()[:12]

def registry_build_from_csv_paths(csv_paths: List[str]) -> Dict[str, Any]:
    mapping = {p: _stable_dataset_id(p) for p in csv_paths}
    reg = {
        "created_ts": time.strftime("%Y-%m-%d %H:%M:%S"),
        "path_to_dataset_id": mapping,
        "latest_path": list(mapping.keys())[-1],
        "latest_dataset_id": list(mapping.values())[-1],
    }
    with open(REGISTRY_PATH, "w", encoding="utf-8") as f:
        json.dump(reg, f, ensure_ascii=False, indent=2)
    return reg

def registry_load() -> Optional[Dict[str, Any]]:
    if not os.path.exists(REGISTRY_PATH):
        return None
    try:
        with open(REGISTRY_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def store_dataset_artifacts(file_path: str, dataset_id: str):
    df = pd.read_csv(file_path)
    artifacts = {
        "dataset_id": dataset_id,
        "file_path": file_path,
        "shape": list(df.shape),
        "columns": list(df.columns),
        "dtypes": {c: str(df[c].dtype) for c in df.columns},
        "head": df.head(5).to_dict(orient="records"),
    }
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    text = f"DATASET COLUMNS {dataset_id}: " + ", ".join(list(df.columns)[:80])
    mid = mem_add_jsonl("dataset_artifacts", text, {"dataset_id": dataset_id, "file_path": file_path, "artifacts": artifacts})
    keyword_index_add(mid, ts, "dataset_artifacts", text, {"dataset_id": dataset_id, "file_path": file_path})
    try:
        semantic_add(mid, text)
    except Exception:
        pass

    kg_add_fact(f"dataset:{dataset_id}", "file_path", file_path, confidence=1.0, meta={}, index_to_memory=False)
    for c in df.columns:
        kg_add_fact(f"dataset:{dataset_id}", "has_column", c, confidence=1.0, meta={}, index_to_memory=False)

# -------------------------
# mm wrapper (Notebook)
# -------------------------
class MM:
    def health_check(self) -> bool:
        os.makedirs(WORK_DIR, exist_ok=True)
        if not os.path.exists(MEM_PATH):
            open(MEM_PATH, "a", encoding="utf-8").close()
        conn = _db_connect()
        conn.close()
        return True

    def rag_add(self, kind: str, text: str, meta: Optional[dict] = None):
        meta = meta or {}
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        mid = mem_add_jsonl(kind, text, meta)
        keyword_index_add(mid, ts, kind, text, meta)
        try:
            semantic_add(mid, text)
        except Exception:
            pass
        return True

    def rag_search(self, query: str, k: int = 5, kind: Optional[str] = None, alpha: float = 0.65):
        kw = keyword_search(query, k=k*2, kind=kind)
        sem = semantic_search(query, k=k*2, kind=kind)

        scores = {}
        for r in kw:
            scores.setdefault(r["id"], {"kw": 0.0, "sem": 0.0})
            scores[r["id"]]["kw"] = max(scores[r["id"]]["kw"], r["score"])
        for r in sem:
            scores.setdefault(r["id"], {"kw": 0.0, "sem": 0.0})
            scores[r["id"]]["sem"] = max(scores[r["id"]]["sem"], r["score"])

        if not scores:
            return []

        kw_vals = np.array([v["kw"] for v in scores.values()], dtype=np.float32)
        sem_vals = np.array([v["sem"] for v in scores.values()], dtype=np.float32)

        kw_norm = (kw_vals - kw_vals.min()) / (kw_vals.max() - kw_vals.min() + 1e-12) if kw_vals.size else kw_vals
        sem_norm = (sem_vals - sem_vals.min()) / (sem_vals.max() - sem_vals.min() + 1e-12) if sem_vals.size else sem_vals

        ids = list(scores.keys())
        for i, rid in enumerate(ids):
            scores[rid]["hybrid"] = float(alpha * kw_norm[i] + (1 - alpha) * sem_norm[i])

        ranked = sorted(ids, key=lambda rid: scores[rid]["hybrid"], reverse=True)[:k]
        all_rows = _mem_load_raw()
        id_to_row = {r.get("id"): r for r in all_rows}

        return [(scores[rid]["hybrid"], id_to_row.get(rid, {"id": rid, "kind": kind, "text": ""})) for rid in ranked]

    def rag_get_latest_registry(self) -> Dict[str, Any]:
        reg = registry_load()
        if reg is None:
            raise RuntimeError(f"Registry not found at {REGISTRY_PATH}. Run registry_build_from_csv_paths(csv_paths).")
        return reg

    def kg_add_fact(self, subj: str, pred: str, obj: str, confidence: float = 1.0, meta: Optional[dict] = None, index_to_memory: bool = True):
        kg_add_fact(subj, pred, obj, confidence=confidence, meta=meta, index_to_memory=index_to_memory)
        return True

mm = MM()
print("mm.health_check():", mm.health_check())
print("WORK_DIR:", WORK_DIR)

# ============================================================
# 3B) Export mm_runtime.py (AutoGen-safe)
#   IMPORTANT: no triple quotes inside this module string.
# ============================================================
import importlib

MM_MODULE_PATH = os.path.join(WORK_DIR, "mm_runtime.py")

mm_runtime_code = r'''
# Auto-generated hybrid memory module for AutoGen executor
# Usage:
#   from mm_runtime import mm

import os, json, time, hashlib, sqlite3
from typing import Optional, List, Dict, Any, Tuple
import numpy as np
import networkx as nx

WORK_DIR = r"__WORK_DIR__"
MEM_PATH      = os.path.join(WORK_DIR, "memory.jsonl")
DB_PATH       = os.path.join(WORK_DIR, "memory_hybrid.sqlite")
VEC_PATH      = os.path.join(WORK_DIR, "memory_vectors.npz")
KG_PATH       = os.path.join(WORK_DIR, "knowledge_graph.graphml")
REGISTRY_PATH = os.path.join(WORK_DIR, "file_path_registry.json")

def _mem_load_raw() -> List[Dict[str, Any]]:
    if not os.path.exists(MEM_PATH):
        return []
    rows = []
    with open(MEM_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rows.append(json.loads(line))
            except Exception:
                pass
    return rows

def mem_add_jsonl(kind: str, text: str, meta: Optional[dict] = None) -> str:
    meta = meta or {}
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    raw = f"{ts}|{kind}|{text}|{json.dumps(meta, sort_keys=True)}".encode("utf-8")
    memory_id = hashlib.sha256(raw).hexdigest()[:16]
    rec = {"id": memory_id, "ts": ts, "kind": str(kind), "text": str(text).strip(), "meta": meta}
    os.makedirs(WORK_DIR, exist_ok=True)
    if not os.path.exists(MEM_PATH):
        open(MEM_PATH, "a", encoding="utf-8").close()
    with open(MEM_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    return memory_id

def _db_connect() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH)
    conn.execute("PRAGMA journal_mode=WAL;")

    conn.execute("CREATE TABLE IF NOT EXISTS memory_struct (id TEXT PRIMARY KEY, ts TEXT, kind TEXT, text TEXT, meta_json TEXT);")
    conn.execute("CREATE VIRTUAL TABLE IF NOT EXISTS memory_fts USING fts5(id UNINDEXED, kind, text, content='', tokenize='porter');")
    conn.execute("CREATE TABLE IF NOT EXISTS doc_summaries (doc_id TEXT PRIMARY KEY, title TEXT, file_path TEXT, summary TEXT, meta_json TEXT);")
    conn.execute("CREATE TABLE IF NOT EXISTS nodes (node_id TEXT PRIMARY KEY, node_type TEXT, title TEXT, text TEXT, doc_id TEXT, meta_json TEXT);")
    conn.execute("CREATE TABLE IF NOT EXISTS edges (src TEXT, rel TEXT, dst TEXT, meta_json TEXT, PRIMARY KEY (src, rel, dst));")

    conn.commit()
    return conn

def keyword_index_add(memory_id: str, ts: str, kind: str, text: str, meta: dict):
    conn = _db_connect()
    conn.execute(
        "INSERT OR REPLACE INTO memory_struct(id, ts, kind, text, meta_json) VALUES (?, ?, ?, ?, ?)",
        (memory_id, ts, kind, text, json.dumps(meta, ensure_ascii=False))
    )
    conn.execute("INSERT INTO memory_fts(id, kind, text) VALUES (?, ?, ?)", (memory_id, kind, text))
    conn.commit()
    conn.close()

def keyword_search(query: str, k: int = 5, kind: Optional[str] = None) -> List[Dict[str, Any]]:
    conn = _db_connect()
    where_kind = " AND kind = ? " if kind else " "
    params = [query]
    if kind:
        params.append(kind)
    params.append(k)

    sql = (
        "SELECT id, kind, text, bm25(memory_fts) AS score "
        "FROM memory_fts "
        "WHERE memory_fts MATCH ? "
        + where_kind +
        "ORDER BY score "
        "LIMIT ?;"
    )
    rows = conn.execute(sql, params).fetchall()
    conn.close()
    return [{"id": rid, "kind": rkind, "text": rtext, "score": float(-score)} for rid, rkind, rtext, score in rows]

def _load_vec_store() -> Tuple[List[str], np.ndarray]:
    if not os.path.exists(VEC_PATH):
        return [], np.zeros((0, 0), dtype=np.float32)
    data = np.load(VEC_PATH, allow_pickle=True)
    return data["ids"].tolist(), data["vecs"].astype(np.float32)

def _save_vec_store(ids: List[str], vecs: np.ndarray):
    np.savez(VEC_PATH, ids=np.array(ids, dtype=object), vecs=vecs.astype(np.float32))

def _normalize_rows(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return X
    norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12
    return X / norms

def embed_text(text: str) -> np.ndarray:
    try:
        from sentence_transformers import SentenceTransformer
        global _ST_MODEL
        if "_ST_MODEL" not in globals():
            _ST_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
        v = _ST_MODEL.encode([text], normalize_embeddings=True)[0]
        return np.array(v, dtype=np.float32)
    except Exception:
        dim = 512
        v = np.zeros(dim, dtype=np.float32)
        for token in text.lower().split():
            h = int(hashlib.sha256(token.encode()).hexdigest(), 16)
            v[h % dim] += 1.0
        v = v / (np.linalg.norm(v) + 1e-12)
        return v

def semantic_add(memory_id: str, text: str):
    ids, vecs = _load_vec_store()
    v = embed_text(text).reshape(1, -1)
    if vecs.size == 0:
        ids = [memory_id]
        vecs = v
    else:
        if v.shape[1] != vecs.shape[1]:
            raise ValueError(f"Embedding dim mismatch: got {v.shape[1]} vs store {vecs.shape[1]}")
        ids.append(memory_id)
        vecs = np.vstack([vecs, v])
    _save_vec_store(ids, vecs)

def semantic_search(query: str, k: int = 5, kind: Optional[str] = None) -> List[Dict[str, Any]]:
    ids, vecs = _load_vec_store()
    if vecs.size == 0:
        return []
    q = embed_text(query).reshape(1, -1).astype(np.float32)
    vecs_n = _normalize_rows(vecs)
    q_n = _normalize_rows(q)
    sims = (vecs_n @ q_n.T).ravel()
    top = np.argsort(-sims)[: min(k * 5, len(ids))]

    id2kind = None
    if kind and len(top) > 0:
        conn = _db_connect()
        ph = ",".join(["?"] * len(top))
        rows = conn.execute(f"SELECT id, kind FROM memory_struct WHERE id IN ({ph})", [ids[i] for i in top]).fetchall()
        conn.close()
        id2kind = {rid: rk for rid, rk in rows}

    out = []
    for i in top:
        rid = ids[i]
        if kind and id2kind and (id2kind.get(rid) != kind):
            continue
        out.append({"id": rid, "score": float(sims[i])})
        if len(out) >= k:
            break
    return out

def registry_load() -> Optional[Dict[str, Any]]:
    if not os.path.exists(REGISTRY_PATH):
        return None
    try:
        with open(REGISTRY_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def health_check() -> bool:
    os.makedirs(WORK_DIR, exist_ok=True)
    if not os.path.exists(MEM_PATH):
        open(MEM_PATH, "a", encoding="utf-8").close()
    conn = _db_connect()
    conn.close()
    return True

class MM:
    def health_check(self) -> bool:
        return health_check()

    def rag_add(self, kind: str, text: str, meta: Optional[dict] = None):
        meta = meta or {}
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        mid = mem_add_jsonl(kind, text, meta)
        keyword_index_add(mid, ts, kind, text, meta)
        try:
            semantic_add(mid, text)
        except Exception:
            pass
        return True

    def rag_search(self, query: str, k: int = 5, kind: Optional[str] = None, alpha: float = 0.65):
        kw = keyword_search(query, k=k*2, kind=kind)
        sem = semantic_search(query, k=k*2, kind=kind)

        scores = {}
        for r in kw:
            scores.setdefault(r["id"], {"kw": 0.0, "sem": 0.0})
            scores[r["id"]]["kw"] = max(scores[r["id"]]["kw"], r["score"])
        for r in sem:
            scores.setdefault(r["id"], {"kw": 0.0, "sem": 0.0})
            scores[r["id"]]["sem"] = max(scores[r["id"]]["sem"], r["score"])

        if not scores:
            return []

        kw_vals = np.array([v["kw"] for v in scores.values()], dtype=np.float32)
        sem_vals = np.array([v["sem"] for v in scores.values()], dtype=np.float32)

        kw_norm = (kw_vals - kw_vals.min()) / (kw_vals.max() - kw_vals.min() + 1e-12) if kw_vals.size else kw_vals
        sem_norm = (sem_vals - sem_vals.min()) / (sem_vals.max() - sem_vals.min() + 1e-12) if sem_vals.size else sem_vals

        ids = list(scores.keys())
        for i, rid in enumerate(ids):
            scores[rid]["hybrid"] = float(alpha * kw_norm[i] + (1 - alpha) * sem_norm[i])

        ranked = sorted(ids, key=lambda rid: scores[rid]["hybrid"], reverse=True)[:k]
        all_rows = _mem_load_raw()
        id_to_row = {r.get("id"): r for r in all_rows}
        return [(scores[rid]["hybrid"], id_to_row.get(rid, {"id": rid, "kind": kind, "text": ""})) for rid in ranked]

    def rag_get_latest_registry(self) -> Dict[str, Any]:
        reg = registry_load()
        if reg is None:
            raise RuntimeError(f"Registry not found at {REGISTRY_PATH}. Build it in the notebook first.")
        return reg

mm = MM()
'''

mm_runtime_code = mm_runtime_code.replace("__WORK_DIR__", WORK_DIR)

with open(MM_MODULE_PATH, "w", encoding="utf-8") as f:
    f.write(mm_runtime_code)

print("✅ mm_runtime.py written to:", MM_MODULE_PATH)

if WORK_DIR not in sys.path:
    sys.path.append(WORK_DIR)

import mm_runtime
importlib.reload(mm_runtime)
print("✅ mm_runtime.mm.health_check():", mm_runtime.mm.health_check())

# ============================================================
# 4) Build registry + store artifacts
# ============================================================
reg = registry_build_from_csv_paths(csv_paths)
for p, dsid in reg["path_to_dataset_id"].items():
    store_dataset_artifacts(p, dsid)

mm.rag_add("registry", f"Registry built with {len(reg['path_to_dataset_id'])} datasets.", reg)
mm.rag_add("run_start", f"Run started. CSV files: {csv_paths}", {"csvs": csv_paths})

print("Latest dataset:", reg["latest_dataset_id"])
print("Latest path:", reg["latest_path"])

# ============================================================
# 5) AutoGen / ag2 setup (YOUR agent lineup)
# ============================================================
from autogen import ConversableAgent, UserProxyAgent, GroupChat, GroupChatManager

llm_config = {
    "config_list": [{"model": os.environ["OPENAI_MODEL_NAME"], "api_key": os.environ["OPENAI_API_KEY"]}],
    "temperature": 0.2,
}

user = UserProxyAgent(
    name="user",
    human_input_mode="NEVER",
    default_auto_reply="TERMINATE",
    max_consecutive_auto_reply=0,
    code_execution_config={"work_dir": WORK_DIR, "use_docker": False, "timeout": 900, "last_n_messages": 12},
)

# ---------------------------
# Persistent workspace paths
# ---------------------------
WORK_DIR = "/content/pavement_agentic_workspace"
os.makedirs(WORK_DIR, exist_ok=True)

MEM_PATH = os.path.join(WORK_DIR, "memory.jsonl")
REGISTRY_PATH = os.path.join(WORK_DIR, "file_path_registry.json")
KG_PATH = os.path.join(WORK_DIR, "knowledge_graph.graphml")

# ---------------------------
# MEMORY INSTRUCTIONS (for all agents)
# ---------------------------
MEMORY_INSTRUCTIONS = f"""
Use memory via `mm` to prevent repeats and to support reporting/citations.

Single source of truth order:
1) mm (preferred): mm.health_check(), mm.rag_add(), mm.rag_search(), mm.rag_get_latest_registry(), mm.kg_add_fact()
2) Fallback files: REGISTRY_PATH={REGISTRY_PATH}, MEM_PATH={MEM_PATH}

Anti-loop:
Before repeating any stage, run: mm.rag_search("PIPELINE_STATE_COMPLETED", k=3). If the same state is already completed, advance to the next state.

What coder must store after each run (lightweight meta only; never big DataFrames):
- pipeline_state: PIPELINE_STATE_COMPLETED=<STATE>, dataset_id, file_path, artifacts_saved
- dataset_artifact: columns, dtypes (as strings), shape, head rows (<=5)
- target_candidates: selected_target + present_feature + rules + path to target_candidates.csv
- domain_rules: maintenance delta rule + treatment thickness bins + unit assumptions
- results: model + metrics + plots + inference CSV paths

Paper + citation memory protocol:
- If you load papers (PDF/text/URL), store one record per paper:
  mm.rag_add(kind="paper", text="<Title> (<Year>)", meta={{"title":..., "authors":..., "year":..., "venue":..., "doi":..., "url":..., "summary":..., "key_quotes": [<=3 short quotes], "bibtex_or_apa":...}})
- Reports must cite ONLY papers in memory by referencing meta fields (author-year) and include a references list (APA).
"""

PIPELINE_STATES = [
  "DISCOVER_DATASET",
  "LOAD_VALIDATE",
  "EDA",
  "POTENTIAL TARGET SELECTION"
  "SPLIT",
  "TRAIN_TUNE",
  "EVALUATE",
  "VISUALIZE",
  "EXPLAIN",
  "PAPER_INGEST",
  "REPORT",
  "STOP"
]

planner_message = f"""
ROLE: Planner/Orchestrator (Agent-0). Control the workflow and decisions.

GOALS:
- Select a defensible target for pavement roughness condition prediction.
- Support maintenance signal + treatment inference when fields allow.
- Ensure reproducible artifacts + decision log stored in memory.

RULES:
- Never run code; delegate to coder_agent.
- Before any decision, consult memory:
  - mm.rag_search("PIPELINE_STATE_COMPLETED", k=3)
  - mm.rag_search(kind="dataset_artifact", query="COLUMNS", k=3)
  - mm.rag_search(kind="target_candidates", query="selected_target", k=3)
- No repeated states twice; if stuck, delegate a different diagnostic.

OUTPUT FORMAT:
STATE_NOW:
DECISIONS:
DELEGATE:
STATE_NEXT:

{MEMORY_INSTRUCTIONS}
"""
pavement_engineer_message = f"""
ROLE: Pavement Engineer (Agent-1). Provide implementable engineering rules.

MUST:
- Read available columns/rules from memory (no invented fields):
  - mm.rag_search(kind="dataset_artifact", query="COLUMNS", k=3)
  - mm.rag_search(kind="domain_rules", query="TREATMENT", k=5)

MAINTENANCE SIGNAL:
- If a paired baseline→future condition exists, define delta = future - baseline.
- Choose/justify epsilon (noise band). If unknown, request data-driven epsilon from coder:
  e.g., small percentile of |delta| on stable segments.

TREATMENT INFERENCE (thickness-driven):
- Identify candidate thickness/overlay/lift fields + unit hints.
- Ask coder to summarize distributions + propose cutpoints (quantiles and/or breaks).
- Select cutpoints and map to treatment labels using domain semantics.
- Store final rule + fields used via mm.rag_add(kind="domain_rules", ...).

OUTPUT FORMAT:
STATE_NOW:
NEW_RULES:
FIELDS_USED:
ASSUMPTIONS:
DELEGATE (to coder_agent):
STATE_NEXT:

{MEMORY_INSTRUCTIONS}
"""

data_scientist_message = f"""
ROLE: Data Scientist (Agent-2). Design modeling plan; coder executes.
You must follow planner_agent, and pavement_engineer_agent to choose exploratroy data analysis, develop and optimize predictive models.

MUST CHECK:
- mm.rag_search(kind="dataset_artifact", query="COLUMNS", k=3)
- mm.rag_search(kind="target_candidates", query="TARGET", k=3)

TARGET:
- Prefer a condition/roughness metric that represents "future" state when possible.
- If only single-time targets exist, use it and treat prediction as cross-sectional.

SPLIT:
- If time column exists: time-aware split.
- If entity/segment/project id exists: group-aware split.
- Else standard split with leakage cautions.

OUTPUT FORMAT (MANDATORY):
STATE_NOW: <one of {PIPELINE_STATES}>
TARGET: <chosen or TBD>
SPLIT_STRATEGY:
  type: <time_aware | group_aware | standard>
  key: <column or NONE>
  rationale: <1-2 lines>
FEATURE_SPEC:
  include: <patterns or list>
  exclude: <leakage columns + IDs if needed>
  preprocessing: <fit-on-train only>
MODEL_SPEC:
  baselines: <short list>
  candidates: <short list>
  final_choice_rule: <1 line>
TUNING_SPEC: <DEFERRED unless TRAIN_TUNE>
METRICS: [R2, RMSE, WMAPE]
DELEGATE (to coder_agent):
STATE_NEXT:

{MEMORY_INSTRUCTIONS}
"""

coder_message = f"""
ROLE: Coder Agent. Execute Python only. One self-contained block per step.
You must follow planner_agent, pavement_engineer_agent, data_scientist_agent for coding tasks. Also use reviewer_agent feedback for any modifications.

NON-NEGOTIABLE:
- Use registry for FILE_PATH (no hardcode).
- Don't reinstall/remount unless error.
- Don't redo completed states unless Planner requests.

MANDATORY PRINTS (end of block):
PIPELINE_STATE_COMPLETED=<STATE>
NEXT_RECOMMENDED_STATE=<STATE>
ARTIFACTS_SAVED=<paths or 'none'>

ALWAYS USE MEMORY:
- Before work: mm.rag_search("PIPELINE_STATE_COMPLETED", k=3)
- After work: mm.rag_add(kind="pipeline_state", text="PIPELINE_STATE_COMPLETED=...", meta={...})
- Store only small summaries in memory; save tables to WORK_DIR.

TARGET DISCOVERY (general):
- Build a scored candidate table from columns using:
  - keyword relevance (iri/roughness/rut/crack/pci/condition/index)
  - numeric validity + coverage
  - future/baseline pairing signals: suffix pairs, "future/next/prev/current", or time columns if present
- Output: WORK_DIR/target_candidates.csv
- Store: mm.rag_add(kind="target_candidates", meta={{selected_target, baseline_feature(if any), scoring_notes, csv_path}})

THICKNESS DISCOVERY (if requested):
- Detect thickness/overlay/lift columns; summarize distributions; propose cutpoints; save thickness_summary.csv and store mm.rag_add(kind="thickness_summary", ...).

TRAIN/EVAL (only when requested by Planner/Data Scientist):
- Save model + params + cv results + test metrics JSON.
- Store paths via mm.rag_add(kind="results", ...)

{MEMORY_INSTRUCTIONS}
"""

reviewer_message = f"""
ROLE: Reviewer (QA). Audit correctness + reproducibility.

CHECK:
- Registry-based FILE_PATH selection
- Completed state artifacts stored in memory
- Split prevents leakage when time/group fields exist
- Metrics are TEST-only
- Treatment inference only when supported by fields + stored rules
- Mandatory end prints exist

OUTPUT FORMAT:
STATUS: <APPROVE/REJECT>
ISSUES:
FIX_REQUEST:
STATE_NEXT: <one of {PIPELINE_STATES}>

{MEMORY_INSTRUCTIONS}
"""

librarian_message = f"""
ROLE: Librarian (Papers + Citations). Build citation-ready memory.

SCOPE:
- Only use documents that exist in memory/files (no invented sources).
- If no papers exist, request ingestion (PDFs/links) via coder_agent.

TASKS:
- Create "citation_pack" entries in memory with:
  - citation_key (e.g., AuthorYear)
  - APA reference
  - 1-2 sentence finding summary
  - optional quote <= 25 words (only if needed)
  - tags (maintenance, IRI, overlays, thickness thresholds, etc.)

OUTPUT FORMAT:
STATE_NOW:
FOUND:
MISSING:
DELEGATE (to coder_agent):
STATE_NEXT:

{MEMORY_INSTRUCTIONS}
"""

researcher_message = f"""
ROLE: Researcher (Technical report + in-text citations).
Write report ONLY from executed artifacts + citation packs in memory.

HARD GATES:
- If results/model/plots missing => respond:
  MISSING_OUTPUTS. DELEGATE: <what coder must store>
- If citations missing and report needs citations => respond:
  MISSING_CITATIONS. DELEGATE: <ask librarian/coder to ingest papers>

MUST RETRIEVE:
- dataset artifacts: mm.rag_search(kind="dataset_artifact", query="DATASET_ID", k=3)
- target + rules: mm.rag_search(kind="target_candidates", query="selected_target", k=3),
                  mm.rag_search(kind="domain_rules", query="MAINTENANCE", k=5),
                  mm.rag_search(kind="domain_rules", query="TREATMENT", k=5)
- results: mm.rag_search(kind="results", query="metrics", k=5)
- citations: mm.rag_search(kind="citation_pack", query="APA", k=10)

REPORT (headings):
1) Dataset and Target
2) Target Selection Logic (autonomous scoring summary)
3) Split Strategy and Leakage Controls
4) Models and Metrics
5) Explainability
6) Maintenance Signal Rules
7) Treatment Inference Rules (thickness-driven)
8) Limitations and Next Steps
9) References (APA)

OUTPUT: report text only.

{MEMORY_INSTRUCTIONS}
"""

# ---------------------------
# Instantiate agents
# ---------------------------
planner_agent = ConversableAgent(
    name="planner_agent",
    system_message=planner_message,
    llm_config=llm_config
)

pavement_engineer_agent = ConversableAgent(
    name="pavement_engineer_agent",
    system_message=pavement_engineer_message,
    llm_config=llm_config
)

data_scientist_agent = ConversableAgent(
    name="data_scientist_agent",
    system_message=data_scientist_message,
    llm_config=llm_config
)

coder_agent = ConversableAgent(
    name="coder_agent",
    system_message=coder_message,
    llm_config=llm_config,
    code_execution_config={
        "work_dir": WORK_DIR,
        "use_docker": False,
        "timeout": 900,
        "last_n_messages": 30,
    },
)

reviewer_agent = ConversableAgent(
    name="reviewer_agent",
    system_message=reviewer_message,
    llm_config=llm_config
)

librarian_agent = ConversableAgent(
    name="librarian_agent",
    system_message=librarian_message,
    llm_config=llm_config
)

researcher_agent = ConversableAgent(
    name="researcher_agent",
    system_message=researcher_message,
    llm_config=llm_config
)

groupchat = GroupChat(
    agents=[
        planner_agent,
        coder_agent,
        pavement_engineer_agent,
        data_scientist_agent,
        reviewer_agent,
        librarian_agent,
        researcher_agent
    ],
    messages=[],
    speaker_selection_method="auto",   # ✅ was round_robin
    max_round=30,
)

def is_termination_msg(msg):
    if not msg:
        return False
    content = msg.get("content", "") if isinstance(msg, dict) else str(msg)

    if ("TERMINATE" in content) or ("PIPELINE_STATE_COMPLETED=STOP" in content):
        return True

    # ✅ anti-loop: stop if generic update gathering repeats
    if "i'm currently gathering updates" in content.lower():
        return True

    return False

manager = GroupChatManager(
    groupchat=groupchat,
    llm_config=llm_config,
    is_termination_msg=is_termination_msg
)

print("✅ AutoGen initialized with agents:",
      "user, planner_agent, pavement_engineer_agent, data_scientist_agent, coder_agent, reviewer_agent, librarian_agent, researcher_agent")

# Kickoff: align states correctly
kickoff_msg = """
STATE_NOW: DISCOVER_DATASET
OBSERVATION:
- Start pipeline by selecting dataset via registry and building dataset artifacts + target candidates.

DELEGATE (to coder_agent):
Run ONE Python block.
- First line: from mm_runtime import mm
- Use mm.rag_get_latest_registry() to select FILE_PATH + DATASET_ID (print all registry entries).
- Load df; store dataset_artifact (columns, dtypes summary, head, signature).
- Run general target discovery (scored ranking) and save target_candidates.csv.
- Store target_candidates in memory.
- End prints:
  PIPELINE_STATE_COMPLETED=DISCOVER_DATASET
  NEXT_RECOMMENDED_STATE=LOAD_VALIDATE
  ARTIFACTS_SAVED=[...]

STATE_NEXT: LOAD_VALIDATE
"""

user.initiate_chat(manager, message=kickoff_msg, max_turns=8)