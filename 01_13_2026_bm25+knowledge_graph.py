# -*- coding: utf-8 -*-
"""01-13-2026-BM25+Knowledge-graph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_7u1bBoWc98A-CA87RuisDbmbYIs3Q4D
"""

!pip -q uninstall -y autogen pyautogen ag2 >/dev/null 2>&1
!pip -q install -U "ag2[openai]" python-dotenv pypdf sentence-transformers >/dev/null 2>&1

!pip show ag2
!python -c "import autogen; print('autogen version:', getattr(autogen,'__version__', 'unknown'))"

import os, json, time, re, hashlib, sqlite3, sys
from dotenv import load_dotenv
load_dotenv()


# Ensure API key exists
os.environ["OPENAI_API_KEY"] =   # set via Secrets
os.environ["OPENAI_MODEL_NAME"] = os.environ.get("OPENAI_MODEL_NAME", "gpt-4o-mini")

print("OPENAI_MODEL_NAME:", os.environ["OPENAI_MODEL_NAME"])
print("OPENAI_API_KEY present?", bool(os.getenv("OPENAI_API_KEY")))

"""## Mount Drive + Set Base Directories (no single FILE_PATH hardcode)"""

from google.colab import drive

if not os.path.exists("/content/drive/MyDrive"):
    drive.mount("/content/drive", force_remount=False)

print("Mounted?", os.path.exists("/content/drive/MyDrive"))

# Base dirs only (no single CSV hardcode)
DATASET_DIR = "/content/drive/MyDrive/Network Maintenace - With & without maintenace/Original-data"
PINNED_FILE_PATH = "/content/drive/MyDrive/Network Maintenace - With & without maintenace/Original-data/2017-2019 (no main).csv"
assert os.path.exists(PINNED_FILE_PATH), f"Pinned CSV not found: {PINNED_FILE_PATH}"

PAPERS_DIR  = "/content/drive/MyDrive/Agentic-AI-Asset-management/Tamim/codes/pavement_papers"

assert os.path.exists(DATASET_DIR), f"DATASET_DIR not found: {DATASET_DIR}"
assert os.path.exists(PAPERS_DIR), f"PAPERS_DIR not found: {PAPERS_DIR}"

print("DATASET_DIR:", DATASET_DIR)
print("PAPERS_DIR:", PAPERS_DIR)

"""## 3. Workspace + Write mm_runtime.py (single importable module)"""

import textwrap

WORK_DIR = "/content/pavement_agentic_workspace"
os.makedirs(WORK_DIR, exist_ok=True)

MM_MODULE_PATH = os.path.join(WORK_DIR, "mm_runtime.py")

mm_runtime_code = r'''
# mm_runtime.py
# Hybrid persistent memory + registry + doc-chunk RAG + optional KG.
# Import: from mm_runtime import mm

import os, re, json, time, glob, hashlib, sqlite3
from typing import Optional, List, Dict, Any, Tuple
import numpy as np

# Optional KG via networkx
try:
    import networkx as nx  # type: ignore
except Exception:
    nx = None

# -------------------------
# Workspace Paths (env override)
# -------------------------
WORK_DIR = os.getenv("WORK_DIR", "/content/pavement_agentic_workspace")
os.makedirs(WORK_DIR, exist_ok=True)

MEM_PATH      = os.path.join(WORK_DIR, "memory.jsonl")
DB_PATH       = os.path.join(WORK_DIR, "memory_hybrid.sqlite")
VEC_PATH      = os.path.join(WORK_DIR, "memory_vectors.npz")
KG_PATH       = os.path.join(WORK_DIR, "knowledge_graph.graphml")
REGISTRY_PATH = os.path.join(WORK_DIR, "file_path_registry.json")

def _now_ts() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S")

def _sha16(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()[:16]

def _stable_file_fingerprint(file_path: str) -> str:
    st = os.stat(file_path)
    raw = f"{file_path}|{st.st_size}|{int(st.st_mtime)}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()[:12]

def _ensure_files():
    os.makedirs(WORK_DIR, exist_ok=True)
    if not os.path.exists(MEM_PATH):
        open(MEM_PATH, "a", encoding="utf-8").close()

# -------------------------
# PDF extraction (pypdf -> fallback PyPDF2)
# -------------------------
def _pdf_extract_text(pdf_path: str) -> str:
    try:
        from pypdf import PdfReader  # type: ignore
        reader = PdfReader(pdf_path)
        texts = []
        for page in reader.pages:
            texts.append(page.extract_text() or "")
        return "\n".join(texts)
    except Exception:
        try:
            import PyPDF2  # type: ignore
            reader = PyPDF2.PdfReader(open(pdf_path, "rb"))
            texts = []
            for page in reader.pages:
                texts.append(page.extract_text() or "")
            return "\n".join(texts)
        except Exception as e:
            raise RuntimeError(f"PDF text extraction failed: {e}")

# -------------------------
# KG
# -------------------------
def _kg_enabled() -> bool:
    return nx is not None

def _kg_load():
    if not _kg_enabled():
        return None
    if os.path.exists(KG_PATH):
        try:
            g = nx.read_graphml(KG_PATH)
            mg = nx.MultiDiGraph()
            mg.add_nodes_from(g.nodes(data=True))
            mg.add_edges_from(g.edges(data=True))
            return mg
        except Exception:
            pass
    return nx.MultiDiGraph()

_KG = _kg_load()

def _kg_save():
    if not _kg_enabled() or _KG is None:
        return
    g = nx.DiGraph()
    g.add_nodes_from(_KG.nodes(data=True))
    for u, v, data in _KG.edges(data=True):
        g.add_edge(u, v, **data)
    nx.write_graphml(g, KG_PATH)

def kg_add_fact(subj: str, pred: str, obj: str, confidence: float = 1.0, meta: Optional[dict] = None):
    if not _kg_enabled() or _KG is None:
        return
    meta = meta or {}
    subj = str(subj); pred = str(pred); obj = str(obj)
    _KG.add_node(subj)
    _KG.add_node(obj)
    _KG.add_edge(subj, obj, relation=pred, confidence=float(confidence), **meta)
    _kg_save()

# -------------------------
# JSONL audit
# -------------------------
def _mem_load_raw() -> List[Dict[str, Any]]:
    if not os.path.exists(MEM_PATH):
        return []
    out = []
    with open(MEM_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                out.append(json.loads(line))
            except Exception:
                pass
    return out

def mem_add_jsonl(kind: str, text: str, meta: Optional[dict] = None) -> str:
    meta = meta or {}
    ts = _now_ts()
    raw = f"{ts}|{kind}|{text}|{json.dumps(meta, sort_keys=True)}"
    memory_id = _sha16(raw)
    rec = {"id": memory_id, "ts": ts, "kind": str(kind), "text": str(text).strip(), "meta": meta}
    _ensure_files()
    with open(MEM_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    return memory_id

# -------------------------
# SQLite hybrid store
# -------------------------
def _db_connect() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH)
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("CREATE TABLE IF NOT EXISTS memory_struct (id TEXT PRIMARY KEY, ts TEXT, kind TEXT, text TEXT, meta_json TEXT);")
    conn.execute("CREATE VIRTUAL TABLE IF NOT EXISTS memory_fts USING fts5(id UNINDEXED, kind, text, content='', tokenize='porter');")
    conn.execute("CREATE TABLE IF NOT EXISTS doc_index (doc_id TEXT PRIMARY KEY, title TEXT, file_path TEXT, doc_type TEXT, tags_json TEXT, meta_json TEXT);")
    conn.execute("CREATE TABLE IF NOT EXISTS doc_chunks (chunk_id TEXT PRIMARY KEY, doc_id TEXT, chunk_index INTEGER, text TEXT, meta_json TEXT);")
    conn.commit()
    return conn

def keyword_index_add(memory_id: str, ts: str, kind: str, text: str, meta: dict):
    conn = _db_connect()
    conn.execute(
        "INSERT OR REPLACE INTO memory_struct(id, ts, kind, text, meta_json) VALUES (?, ?, ?, ?, ?)",
        (memory_id, ts, kind, text, json.dumps(meta, ensure_ascii=False))
    )
    conn.execute("INSERT INTO memory_fts(id, kind, text) VALUES (?, ?, ?)", (memory_id, kind, text))
    conn.commit()
    conn.close()

def keyword_search(query: str, k: int = 5, kind: Optional[str] = None) -> List[Dict[str, Any]]:
    conn = _db_connect()
    where_kind = " AND kind = ? " if kind else " "
    params = [query]
    if kind:
        params.append(kind)
    params.append(k)
    sql = (
        "SELECT id, kind, text, bm25(memory_fts) AS score "
        "FROM memory_fts "
        "WHERE memory_fts MATCH ? "
        + where_kind +
        "ORDER BY score "
        "LIMIT ?;"
    )
    rows = conn.execute(sql, params).fetchall()
    conn.close()
    return [{"id": rid, "kind": rkind, "text": rtext, "score": float(-score)} for rid, rkind, rtext, score in rows]

# -------------------------
# Semantic vectors (NPZ) for memory + doc chunks
# -------------------------
def _load_vec_store() -> Tuple[List[str], np.ndarray]:
    if not os.path.exists(VEC_PATH):
        return [], np.zeros((0, 0), dtype=np.float32)
    data = np.load(VEC_PATH, allow_pickle=True)
    return data["ids"].tolist(), data["vecs"].astype(np.float32)

def _save_vec_store(ids: List[str], vecs: np.ndarray):
    np.savez(VEC_PATH, ids=np.array(ids, dtype=object), vecs=vecs.astype(np.float32))

def _normalize_rows(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return X
    norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12
    return X / norms

def embed_text(text: str) -> np.ndarray:
    try:
        from sentence_transformers import SentenceTransformer  # type: ignore
        global _ST_MODEL
        if "_ST_MODEL" not in globals():
            _ST_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
        v = _ST_MODEL.encode([text], normalize_embeddings=True)[0]
        return np.array(v, dtype=np.float32)
    except Exception:
        dim = 512
        v = np.zeros(dim, dtype=np.float32)
        for token in re.findall(r"[a-z0-9_]+", text.lower()):
            h = int(hashlib.sha256(token.encode()).hexdigest(), 16)
            v[h % dim] += 1.0
        v = v / (np.linalg.norm(v) + 1e-12)
        return v

def semantic_add(item_id: str, text: str):
    ids, vecs = _load_vec_store()
    v = embed_text(text).reshape(1, -1)
    if vecs.size == 0:
        ids = [item_id]
        vecs = v
    else:
        # reset safely on dim mismatch
        if v.shape[1] != vecs.shape[1]:
            ids = [item_id]
            vecs = v
        else:
            ids.append(item_id)
            vecs = np.vstack([vecs, v])
    _save_vec_store(ids, vecs)

def semantic_search_ids(query: str, k: int = 5) -> List[Tuple[str, float]]:
    ids, vecs = _load_vec_store()
    if vecs.size == 0:
        return []
    q = embed_text(query).reshape(1, -1).astype(np.float32)
    sims = (_normalize_rows(vecs) @ _normalize_rows(q).T).ravel()
    top = np.argsort(-sims)[: min(k, len(ids))]
    return [(ids[i], float(sims[i])) for i in top]

# -------------------------
# Registry (scan CSVs; pick latest by mtime)
# -------------------------
def registry_build(dataset_dir: str) -> Dict[str, Any]:
    csvs = sorted(glob.glob(os.path.join(dataset_dir, "**/*.csv"), recursive=True))
    if not csvs:
        raise FileNotFoundError(f"No CSV found under dataset_dir={dataset_dir}")
    mapping = {p: _stable_file_fingerprint(p) for p in csvs}
    latest_path = max(mapping.keys(), key=lambda p: os.path.getmtime(p))
    reg = {
        "created_ts": _now_ts(),
        "path_to_dataset_id": mapping,
        "latest_path": latest_path,
        "latest_dataset_id": mapping[latest_path],
    }
    with open(REGISTRY_PATH, "w", encoding="utf-8") as f:
        json.dump(reg, f, ensure_ascii=False, indent=2)
    return reg

def registry_load() -> Optional[Dict[str, Any]]:
    if not os.path.exists(REGISTRY_PATH):
        return None
    try:
        with open(REGISTRY_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

# -------------------------
# Engineering-aware target discovery: baseline *_x -> future *_y (IRI first)
# -------------------------
_COND_KEYWORDS = [
    ("iri", 5), ("roughness", 5), ("pci", 4), ("rut", 3), ("crack", 3),
    ("fault", 2), ("skid", 2), ("friction", 2), ("condition", 2), ("index", 1),
]

def _score_name(col: str) -> int:
    c = col.lower()
    return sum(w for kw, w in _COND_KEYWORDS if kw in c)

def discover_target_pairs(columns: List[str]) -> List[Dict[str, Any]]:
    x_cols = {c for c in columns if c.lower().endswith("_x")}
    y_cols = {c for c in columns if c.lower().endswith("_y")}
    pairs = []
    for x in sorted(x_cols):
        base = x[:-2]
        y = base + "_y"
        if y in y_cols:
            s = _score_name(base)
            if s <= 0:
                continue
            b = base.lower()
            if "iri" in b or "roughness" in b:
                family, pri = "IRI/Roughness", 100
            elif "pci" in b:
                family, pri = "PCI", 80
            elif "rut" in b:
                family, pri = "Rutting", 60
            elif "crack" in b:
                family, pri = "Cracking", 60
            else:
                family, pri = "Condition", 40
            pairs.append({
                "pair_name": f"{base}_x->y",
                "baseline_col": x,
                "target_col": y,
                "metric_family": family,
                "priority": pri,
                "name_score": s,
                "notes": "Baseline=current (_x), Target=future (_y). Exclude *_y from features.",
            })
    pairs.sort(key=lambda p: -(p["priority"] + p["name_score"]))
    return pairs

# -------------------------
# Doc ingestion + chunk search
# -------------------------
def _chunk_text(text: str, chunk_chars: int = 1200, overlap: int = 200) -> List[str]:
    text = re.sub(r"\s+", " ", text).strip()
    if not text:
        return []
    chunks = []
    i = 0
    while i < len(text):
        j = min(len(text), i + chunk_chars)
        chunks.append(text[i:j])
        if j == len(text):
            break
        i = max(0, j - overlap)
    return chunks

def doc_add_pdf(file_path: str, title: Optional[str] = None, doc_type: str = "paper",
                tags: Optional[List[str]] = None, meta: Optional[dict] = None,
                chunk_chars: int = 1200, overlap: int = 200) -> str:
    tags = tags or []
    meta = meta or {}
    title = title or os.path.basename(file_path)

    doc_fp = _stable_file_fingerprint(file_path)
    doc_id = f"{doc_type}:{doc_fp}"

    text = _pdf_extract_text(file_path)
    chunks = _chunk_text(text, chunk_chars=chunk_chars, overlap=overlap)

    conn = _db_connect()
    conn.execute(
        "INSERT OR REPLACE INTO doc_index(doc_id, title, file_path, doc_type, tags_json, meta_json) VALUES (?, ?, ?, ?, ?, ?)",
        (doc_id, title, file_path, doc_type, json.dumps(tags), json.dumps(meta)),
    )
    conn.execute("DELETE FROM doc_chunks WHERE doc_id = ?", (doc_id,))

    for idx, ch in enumerate(chunks):
        chunk_id = f"chunk:{doc_fp}:{idx}"
        conn.execute(
            "INSERT OR REPLACE INTO doc_chunks(chunk_id, doc_id, chunk_index, text, meta_json) VALUES (?, ?, ?, ?, ?)",
            (chunk_id, doc_id, idx, ch, json.dumps({"tags": tags, "title": title}, ensure_ascii=False)),
        )
        semantic_add(chunk_id, ch)

    conn.commit()
    conn.close()

    kg_add_fact(doc_id, "file_path", file_path, confidence=1.0)
    for t in tags:
        kg_add_fact(doc_id, "tag", t, confidence=1.0)

    # lightweight memory record
    mid = mem_add_jsonl("paper", f"{title} ingested with {len(chunks)} chunks.", {"doc_id": doc_id, "file_path": file_path, "tags": tags})
    keyword_index_add(mid, _now_ts(), "paper", f"{title} ingested with {len(chunks)} chunks.", {"doc_id": doc_id, "file_path": file_path})
    semantic_add(mid, f"{title} {tags}")

    return doc_id

def doc_search(query: str, k: int = 5, tags: Optional[List[str]] = None) -> List[Dict[str, Any]]:
    tags = tags or []
    scored = semantic_search_ids(query, k=max(25, k*5))
    if not scored:
        return []
    conn = _db_connect()
    out = []
    for chunk_id, score in scored:
        if not chunk_id.startswith("chunk:"):
            continue
        row = conn.execute("SELECT chunk_id, doc_id, chunk_index, text, meta_json FROM doc_chunks WHERE chunk_id = ?", (chunk_id,)).fetchone()
        if not row:
            continue
        chunk_id2, doc_id, chunk_index, text, meta_json = row
        meta = json.loads(meta_json) if meta_json else {}
        if tags:
            mtags = meta.get("tags", [])
            if not any(t in mtags for t in tags):
                continue
        doc_row = conn.execute("SELECT title, file_path, doc_type, tags_json FROM doc_index WHERE doc_id = ?", (doc_id,)).fetchone()
        if not doc_row:
            continue
        title, file_path, doc_type, tags_json = doc_row
        dtags = json.loads(tags_json) if tags_json else []
        out.append({
            "score": float(score),
            "chunk_id": chunk_id2,
            "doc_id": doc_id,
            "chunk_index": int(chunk_index),
            "title": title,
            "file_path": file_path,
            "doc_type": doc_type,
            "doc_tags": dtags,
            "text": text,
        })
        if len(out) >= k:
            break
    conn.close()
    return out

# -------------------------
# Public API
# -------------------------
class MM:
    def health_check(self) -> bool:
        _ensure_files()
        conn = _db_connect()
        conn.close()
        return True

    def rag_add(self, kind: str, text: str, meta: Optional[dict] = None) -> bool:
        meta = meta or {}
        ts = _now_ts()
        mid = mem_add_jsonl(kind, text, meta)
        keyword_index_add(mid, ts, kind, text, meta)
        semantic_add(mid, text)
        return True

    def rag_search(self, query: str, k: int = 5, kind: Optional[str] = None, alpha: float = 0.65):
        kw = keyword_search(query, k=k*2, kind=kind)
        sem_ids = semantic_search_ids(query, k=k*2)
        sem_map = {rid: sc for rid, sc in sem_ids}

        scores: Dict[str, Dict[str, float]] = {}
        for r in kw:
            scores.setdefault(r["id"], {"kw": 0.0, "sem": 0.0})
            scores[r["id"]]["kw"] = max(scores[r["id"]]["kw"], r["score"])
        for rid, sc in sem_map.items():
            scores.setdefault(rid, {"kw": 0.0, "sem": 0.0})
            scores[rid]["sem"] = max(scores[rid]["sem"], sc)

        if not scores:
            return []

        kw_vals = np.array([v["kw"] for v in scores.values()], dtype=np.float32)
        sem_vals = np.array([v["sem"] for v in scores.values()], dtype=np.float32)
        kw_norm = (kw_vals - kw_vals.min()) / (kw_vals.max() - kw_vals.min() + 1e-12) if kw_vals.size else kw_vals
        sem_norm = (sem_vals - sem_vals.min()) / (sem_vals.max() - sem_vals.min() + 1e-12) if sem_vals.size else sem_vals

        ids = list(scores.keys())
        for i, rid in enumerate(ids):
            scores[rid]["hybrid"] = float(alpha * kw_norm[i] + (1 - alpha) * sem_norm[i])

        ranked = sorted(ids, key=lambda rid: scores[rid]["hybrid"], reverse=True)[:k]
        all_rows = _mem_load_raw()
        id_to_row = {r.get("id"): r for r in all_rows}
        return [(scores[rid]["hybrid"], id_to_row.get(rid, {"id": rid, "kind": kind, "text": ""})) for rid in ranked]

    def registry_build(self, dataset_dir: str) -> Dict[str, Any]:
        return registry_build(dataset_dir)

    def rag_get_latest_registry(self) -> Dict[str, Any]:
        reg = registry_load()
        if reg is None:
            raise RuntimeError("Registry not found. Run mm.registry_build(dataset_dir) first.")
        return reg

    def discover_target_pairs(self, columns: List[str]) -> List[Dict[str, Any]]:
        return discover_target_pairs(columns)

    def doc_add_pdf(self, file_path: str, title: Optional[str] = None, doc_type: str = "paper", tags: Optional[List[str]] = None, meta: Optional[dict] = None) -> str:
        return doc_add_pdf(file_path=file_path, title=title, doc_type=doc_type, tags=tags, meta=meta)

    def doc_search(self, query: str, k: int = 5, tags: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        return doc_search(query=query, k=k, tags=tags)

    def kg_add_fact(self, subj: str, pred: str, obj: str, confidence: float = 1.0, meta: Optional[dict] = None) -> bool:
        kg_add_fact(subj, pred, obj, confidence=confidence, meta=meta)
        return True


mm = MM()
'''

with open(MM_MODULE_PATH, "w", encoding="utf-8") as f:
    f.write(textwrap.dedent(mm_runtime_code).lstrip())

if WORK_DIR not in sys.path:
    sys.path.append(WORK_DIR)

from mm_runtime import mm
print("✅ mm_runtime.py written to:", MM_MODULE_PATH)
print("✅ mm.health_check():", mm.health_check())

"""## 4) Build registry (scan DATASET_DIR) + (optional) quick dataset preview"""

import pandas as pd
from mm_runtime import mm

import json, hashlib, os, time

def force_registry_latest_to_pinned(work_dir: str, pinned_path: str):
    reg_path = os.path.join(work_dir, "file_path_registry.json")
    st = os.stat(pinned_path)
    raw = f"{pinned_path}|{st.st_size}|{int(st.st_mtime)}".encode("utf-8")
    dsid = hashlib.sha256(raw).hexdigest()[:12]

    reg = {
        "created_ts": time.strftime("%Y-%m-%d %H:%M:%S"),
        "path_to_dataset_id": {pinned_path: dsid},
        "latest_path": pinned_path,
        "latest_dataset_id": dsid,
        "pinned": True
    }
    with open(reg_path, "w", encoding="utf-8") as f:
        json.dump(reg, f, indent=2)
    print("✅ Registry pinned to:", pinned_path)
    print("✅ dataset_id:", dsid)

force_registry_latest_to_pinned(WORK_DIR, PINNED_FILE_PATH)

reg = mm.rag_get_latest_registry()
print("Registry loaded (PINNED):", reg["created_ts"])
print("Latest dataset_id:", reg["latest_dataset_id"])
print("Latest path:", reg["latest_path"])

# lightweight preview (NOT stored fully in memory)
FILE_PATH = reg["latest_path"]
DATASET_ID = reg["latest_dataset_id"]
df = pd.read_csv(FILE_PATH)

print("df.shape:", df.shape)
print("df.columns (first 60):", list(df.columns)[:60])
display(df.head(3))

"""## 4B) Create target_candidates.csv (baseline *_x -> target *_y; prefer IRI)"""

import os
import pandas as pd
from mm_runtime import mm

pairs = mm.discover_target_pairs(list(df.columns))

rows = []
for p in pairs:
    b = p["baseline_col"]
    y = p["target_col"]
    rows.append({
        **p,
        "baseline_coverage": float(df[b].notna().mean()),
        "target_coverage": float(df[y].notna().mean()),
        "leakage_exclude_rule": "Exclude ALL *_y columns from features except chosen target.",
        "feature_include_rule": "Baseline *_x is allowed as predictor (e.g., IRI_mean_x).",
    })

target_candidates_df = pd.DataFrame(rows)
TARGET_CSV = os.path.join(WORK_DIR, "target_candidates.csv")
target_candidates_df.to_csv(TARGET_CSV, index=False)

recommended = None
if not target_candidates_df.empty:
    tmp = target_candidates_df.copy()
    tmp["score2"] = tmp["priority"].fillna(0) + 50*tmp["target_coverage"].fillna(0) + 10*tmp["baseline_coverage"].fillna(0)
    recommended = tmp.sort_values("score2", ascending=False).head(1).to_dict(orient="records")[0]

mm.rag_add("target_candidates", f"Saved target candidates at {TARGET_CSV}", {
    "dataset_id": DATASET_ID,
    "file_path": FILE_PATH,
    "csv_path": TARGET_CSV,
    "n": int(len(target_candidates_df)),
    "recommended": recommended,
    "rule": "Prefer paired *_x -> *_y. For roughness, predict IRI*_y using IRI*_x + covariates.",
})

print("Saved:", TARGET_CSV)
print("Top recommended:", recommended)

"""## 4C) Ingest papers (2 HPMS + 1 LTPP) into doc-chunk RAG + KG tags"""

import os, glob
from mm_runtime import mm

pdfs = sorted(glob.glob(os.path.join(PAPERS_DIR, "*.pdf")))
assert pdfs, f"No PDFs found in {PAPERS_DIR}"

def tag_pdf(title: str):
    tl = title.lower()
    if "automation-in-construction" in tl:
        return ["HPMS", "IRI", "roughness"]
    if "results-in-engineering" in tl:
        return ["LTPP", "IRI", "roughness"]
    if "ghodratabadi" in tl:
        return ["HPMS", "IRI", "roughness", "maintenance"]
    # fallback
    return ["HPMS", "IRI", "roughness"]

ingested = []
for p in pdfs:
    title = os.path.basename(p)
    tags = tag_pdf(title)
    doc_id = mm.doc_add_pdf(p, title=title, tags=tags, meta={"papers_dir": PAPERS_DIR})
    ingested.append({"doc_id": doc_id, "title": title, "tags": tags})

mm.rag_add("papers_ingested", "Ingested papers into doc-chunk RAG.", {
    "papers_dir": PAPERS_DIR,
    "n_pdfs": len(pdfs),
    "ingested": ingested
})

# quick retrieval sanity test
hpms_hits = mm.doc_search("predict future IRI using baseline IRI", tags=["HPMS"], k=3)
ltpp_hits = mm.doc_search("roughness progression and maintenance effect", tags=["LTPP"], k=3)

print("HPMS hits:")
for h in hpms_hits:
    print("-", h["title"], "| score", round(h["score"], 3), "| chunk", h["chunk_index"])
    print(" ", h["text"][:240], "...\n")

print("LTPP hits:")
for h in ltpp_hits:
    print("-", h["title"], "| score", round(h["score"], 3), "| chunk", h["chunk_index"])
    print(" ", h["text"][:240], "...\n")

"""## 5) AutoGen / ag2 setup (cleaner prompts + fixed PIPELINE_STATES)"""

from autogen import ConversableAgent, UserProxyAgent, GroupChat, GroupChatManager
from mm_runtime import mm

llm_config = {
    "config_list": [{"model": os.environ["OPENAI_MODEL_NAME"], "api_key": os.environ["OPENAI_API_KEY"]}],
    "temperature": 0.2,
}

user = UserProxyAgent(
    name="user",
    human_input_mode="NEVER",
    default_auto_reply="TERMINATE",
    max_consecutive_auto_reply=0,
    code_execution_config={"work_dir": WORK_DIR, "use_docker": False, "timeout": 900, "last_n_messages": 12},
)

from autogen import ConversableAgent, UserProxyAgent, GroupChat, GroupChatManager
from mm_runtime import mm

llm_config = {
    "config_list": [{"model": os.environ["OPENAI_MODEL_NAME"], "api_key": os.environ["OPENAI_API_KEY"]}],
    "temperature": 0.2,
}

user = UserProxyAgent(
    name="user",
    human_input_mode="NEVER",
    default_auto_reply="TERMINATE",
    max_consecutive_auto_reply=0,
    code_execution_config={"work_dir": WORK_DIR, "use_docker": False, "timeout": 900, "last_n_messages": 12},
)

PIPELINE_STATES = [
  "DISCOVER_DATASET",
  "PAPER_INGEST",
  "LOAD_VALIDATE",
  "TARGET_SELECT",
  "SPLIT",
  "TRAIN_EVAL",
  "MODEL_SELECT",
  "REPORT",
  "STOP"
]

MEMORY_INSTRUCTIONS = f"""
Use mm memory for repeat-avoidance and auditable artifacts.

HARD RULES:
- Every coder block MUST start with:
  from mm_runtime import mm
  mm.health_check()
- Before repeating any state, run:
  mm.rag_search("PIPELINE_STATE_COMPLETED=<STATE>", kind="pipeline_state", k=5)
  If found for same dataset_id, do NOT rerun unless Planner explicitly requests.

ENGINEERING TARGET RULE:
- Prefer paired condition metrics: *_x (baseline/current) -> *_y (future).
- For roughness, default target family is IRI/Roughness:
  Predict IRI*_y using IRI*_x + covariates.

LEAKAGE RULE (non-negotiable):
- Exclude ALL *_y columns from features except the selected target *_y.

ARTIFACT RULE:
- Each state must save required files in WORK_DIR and write a lightweight mm record:
  kind="pipeline_state" with PIPELINE_STATE_COMPLETED=<STATE> and artifacts_saved paths.

PINNED DATASET RULE:
- For this run, ALWAYS use reg["latest_path"] (it is pinned to PINNED_FILE_PATH).
- Do NOT scan DATASET_DIR for a different latest file unless Planner explicitly disables pinning.

CONTEXT PATHS:
WORK_DIR={WORK_DIR}
DATASET_DIR={DATASET_DIR}
PAPERS_DIR={PAPERS_DIR}
PINNED_FILE_PATH={PINNED_FILE_PATH}
""".strip()

planner_message = f"""
ROLE: You are the planner_agent (Agent-0) in an AutoGen to orchestrate the multi-agent system for pavement condition analysis.
Goal: You drive the workflow end-to-end to build a future pavement condition predictor by making evidence-based decisions and coordinating other agents. you should prefer IRI*_y as target using baseline IRI*_x + covariates.
You should guide the data_scientist_agent to be compatible for the variables from multiple database including HPMS and LTPP. Then the  the data_scientist_agent can tell the coder agent to develop and execute codes accordingly.
You can use ingested papers who utlized data from HPMS/LTPP via mm.doc_search to justify target + modeling choices.

YOU DO NOT RUN CODE. you deleg

HARD GATES (must be satisfied by coder_agent execution; verify by file existence + mm records):
- Registry must remain pinned; do not rebuild registry from DATASET_DIR unless explicitly requested.
- After DISCOVER_DATASET:
  - WORK_DIR/target_candidates.csv exists
  - mm record kind="target_candidates" exists for this dataset_id
- After PAPER_INGEST:
  - at least 3 ingested papers recorded (mm kind="paper" or "papers_ingested")
  - mm.doc_search returns at least 1 hit for tags=["HPMS"] and tags=["LTPP"]
- After LOAD_VALIDATE:
  - WORK_DIR/validate_report.json exists
- After TRAIN_EVAL:
  - WORK_DIR/metrics.json exists
  - WORK_DIR/metrics.csv exists
  - WORK_DIR/best_model.pkl exists
  - mm record kind="results" contains those paths
- After MODEL_SELECT:
  - WORK_DIR/model_card.md exists
- After REPORT:
  - WORK_DIR/report.md exists

If any gate is missing:
- Do NOT advance state.
- Delegate coder_agent to rerun ONLY the missing artifact step.


{MEMORY_INSTRUCTIONS}
""".strip()

pavement_engineer_message = f"""
ROLE: You are the pavement engineer (Agent-1).
Purpose: Translate columns into engineering meaning and define defensible rules such as pavement-maintenance interpretations and implementable treatment rules.

YOU DO NOT RUN CODE.
You must base reasoning on real dataset fields retrieved from memory:

Must retrieve before advising:
- mm.rag_search(kind="dataset_artifact", query="COLUMNS", k=3) if available
- mm.rag_search(kind="target_candidates", query="Saved target candidates", k=3)
- mm.doc_search("IRI", tags=["HPMS"], k=3)
- mm.doc_search("IRI", tags=["LTPP"], k=3)

Provide:
- Engineering interpretation of baseline vs future condition (IRI*_x -> IRI*_y)
- Leakage warnings (exclude *_y)
- If treatment fields exist, suggest candidate treatment indicators (overlay/thickness), but DO NOT invent fields.
Before proposing thresholds/mappings:
- Search for prior rules using mm.rag_search("treatment_thresholds", ...) or related queries.


{MEMORY_INSTRUCTIONS}
""".strip()

data_scientist_message = f"""
ROLE: You are the data scientist (Agent-2) for predicting pavement condition through developing appropriate predictive modeling and data analytics.

Purpose:
- You follow the planner_agent (Agent-0) and pavement engineer (Agent-1) to specify the modeling plan.
- You must guide the coder agent to load the real dataset.
- You must never push the coder agent to generate synthetic datasets for analysis, model development, or evaluation tasks.
- You must delegate your decisions to the coder agent (Agent-3) to select target variables and input features with appropriate reasoning based on exploratory data analysis.
- You must also delegate your decisions to the coder agent (Agent-3) to execute code for predictive model development with necessary preprocessing, optimization, prediction, and visualizations.
- You should explore multiple models so that the predictive model can achieve the highest possible accuracy.
- Feature engineering is important to ensure that no useful features are excluded unnecessarily.

YOU DO NOT RUN CODE.

Must retrieve:
- mm.rag_search(kind="target_candidates", query="csv_path", k=3)
- mm.rag_search(kind="dataset_artifact", query="COLUMNS", k=3)
- mm.doc_search("predict pavement roughness IRI", tags=["HPMS"], k=3)
- mm.doc_search("predict pavement roughness IRI", tags=["LTPP"], k=3)

Rules:
- Target must be a future *_y variable; baseline *_x can be used as a feature.
- Exclude all *_y variables from features except the selected target.
- Split strategy:
  - If a real time column exists: sort by time, then apply a time-based split.
  - Otherwise: apply shuffle=False split as a proxy for time order.
- Metrics (test-only): R2, RMSE, WMAPE.

OUTPUT (strict):
STATE_NOW: <one of PIPELINE_STATES>
TARGET:
SPLIT_STRATEGY:
FEATURE_RULES:
MODELS:
DELEGATE (to coder_agent):
STATE_NEXT:

{MEMORY_INSTRUCTIONS}
""".strip()

coder_message = f"""
ROLE: You are the coder agent (Agent-3).
You follow the data scientist (Agent-2) to develop code for pavement condition analysis through exploratory analysis, preprocessing, modeling, and optimization.

You execute Python ONLY.
Exactly ONE self-contained Python block per state.
Assume each block is stateless and does not rely on variables from prior states.
Persist everything as files in WORK_DIR and lightweight mm records.

NON-NEGOTIABLE:
- The first line of every block must be: from mm_runtime import mm
- Always call: mm.health_check()
- Always use the registry (no hardcoded FILE_PATH):
  reg = mm.rag_get_latest_registry()
  FILE_PATH = reg["latest_path"]
  DATASET_ID = reg["latest_dataset_id"]
- If the registry is missing:
  - If PINNED_FILE_PATH is available, create a pinned registry using it.
  - Otherwise, build the registry using mm.registry_build(DATASET_DIR).
- Save all outputs to WORK_DIR only.
- Never claim a state is completed unless required artifacts exist AND mm.rag_add has been written.

GLOBAL ENGINEERING RULES:
- Future prediction must follow baseline *_x → target *_y (prefer IRI / Roughness).
- Leakage control is mandatory: exclude ALL *_y variables from features except the selected target.
- Baseline *_x of the same metric is allowed as a feature.

STATE REQUIREMENTS:

(1) DISCOVER_DATASET:
- Load the dataset from FILE_PATH.
- Build ranked target_candidates.csv with columns:
  pair_name, baseline_col, target_col, metric_family,
  baseline_coverage, target_coverage, leakage_rule, notes
- Prefer IRI / Roughness pairs.
- Save: WORK_DIR/target_candidates.csv
- Write mm records:
  - kind="target_candidates" with csv_path, dataset_id, file_path, recommended_pair
  - kind="pipeline_state" with PIPELINE_STATE_COMPLETED=DISCOVER_DATASET

(2) PAPER_INGEST:
- Ingest PDFs under PAPERS_DIR.
- Tag each PDF based on filename rules:
  - Automation-in-construction → HPMS
  - Results-in-engineering → LTPP
  - Ghodratabadi → HPMS
- Add PDFs using mm.doc_add_pdf(...)
- Run sanity checks:
  - mm.doc_search("IRI prediction", tags=["HPMS"], k=2)
  - mm.doc_search("IRI prediction", tags=["LTPP"], k=2)
- Write mm records:
  - kind="papers_ingested" with file list and tags
  - kind="pipeline_state" with PIPELINE_STATE_COMPLETED=PAPER_INGEST

(3) LOAD_VALIDATE:
- Create validate_report.json containing:
  - dataset_id, file_path, dataset shape
  - top 30 missingness summary
  - numeric range checks for baseline and target candidates
- Save: WORK_DIR/validate_report.json
- Write mm record:
  - kind="pipeline_state" with PIPELINE_STATE_COMPLETED=LOAD_VALIDATE

(4) TARGET_SELECT:
- Read WORK_DIR/target_candidates.csv.
- Select the best IRI / Roughness pair using:
  priority = IRI first, then highest target coverage.
- Save: WORK_DIR/selected_target.json
- Write mm records:
  - kind="target_selected" with selected baseline, target, and rationale
  - kind="pipeline_state" with PIPELINE_STATE_COMPLETED=TARGET_SELECT

(5) FEATURE_ENGINEERING:
- Select input features.
- Print selected features.
- Show correlations.
- Provide reasoning for inclusion and exclusion of features.

(6) SPLIT:
- Detect time-related columns (year, date, letting_year, etc.).
- If found: sort by time.
- Otherwise: use shuffle=False split.
- Save: WORK_DIR/split_plan.json
- Write mm record:
  - kind="pipeline_state" with PIPELINE_STATE_COMPLETED=SPLIT

(7) TRAIN_EVAL (MANDATORY):
- Load dataset and selected target.
- Build feature matrix with leakage control.
- Exclude ID-like columns.
- Apply split plan.
- Train:
  LinearRegression, RandomForestRegressor, GradientBoostingRegressor.
- Compute TEST metrics: R2, RMSE, WMAPE.
- Print metrics to console.
- Select best model based on lowest RMSE.
- Save:
  WORK_DIR/metrics.csv,
  WORK_DIR/metrics.json,
  WORK_DIR/best_model.pkl
- Write mm records:
  - kind="results"
  - kind="pipeline_state" with PIPELINE_STATE_COMPLETED=TRAIN_EVAL

(8) MODEL_SELECT:
- Create WORK_DIR/model_card.md summarizing:
  target, features, leakage rule, split rule,
  best model, metrics, limitations.
- Write mm record:
  - kind="pipeline_state" with PIPELINE_STATE_COMPLETED=MODEL_SELECT

(9) REPORT:
- Create WORK_DIR/report.md using only:
  validate_report.json,
  selected_target.json,
  split_plan.json,
  metrics.json/csv,
  model_card.md,
  and mm.doc_search snippets (HPMS/LTPP).
- Write mm record:
  - kind="pipeline_state" with PIPELINE_STATE_COMPLETED=REPORT

MANDATORY END PRINTS:
PIPELINE_STATE_COMPLETED=<STATE>
NEXT_RECOMMENDED_STATE=<STATE>
ARTIFACTS_SAVED=<paths or 'none'>

{MEMORY_INSTRUCTIONS}
""".strip()

reviewer_message = f"""
ROLE: You are the Reviewer Agent (Agent-4) for quality assurance (QA).

Purpose:
- Provide feedback to the planner_agent (Agent-0),
  pavement engineer (Agent-1),
  data scientist (Agent-2),
  and coder agent (Agent-3).
- Ensure predictive model correctness, real artifact generation,
  and strict leakage control.

Must check:
- Registry-based FILE_PATH is used (no hardcoding).
- Target selection follows *_y and baseline *_x exists.
- Leakage rule is enforced (exclude *_y features except target).
- TRAIN_EVAL prints R2, RMSE, and WMAPE.
- Required artifacts exist on disk for the completed state.
- mm pipeline_state records exist.

OUTPUT (strict):
STATUS: <APPROVE / REJECT>
ISSUES:
FIX_REQUEST:
STATE_NEXT:

{MEMORY_INSTRUCTIONS}
""".strip()

librarian_message = f"""
ROLE: you are the librarian Agent (Agent-5).
Purpose: Create citation_pack entries ONLY from ingested PDFs (no invented sources).

Must use:
- mm.doc_search(query, tags=["HPMS"] or ["LTPP"])

Deliverable:
- mm.rag_add(kind="citation_pack", text="...", meta={{citation_key, pdf_filename, short_finding, snippet, tags}})

OUTPUT (strict):
STATE_NOW:
FOUND:
MISSING:
DELEGATE (to coder_agent):
STATE_NEXT:

{MEMORY_INSTRUCTIONS}
""".strip()

researcher_message = f"""
ROLE: You are the Researcher and Reporter (Agent-6).
Purpose: Write the final technical report: dataset description, target selection, maintenance inference logic,
treatment thresholds, modeling approach, results, limitations, and conclusions.from saved artifacts + ingested PDFs.

HARD GATE:
If any are missing, respond ONCE then STOP:
- WORK_DIR/metrics.json
- WORK_DIR/metrics.csv
- WORK_DIR/model_card.md

If missing:
MISSING_OUTPUTS. DELEGATE (to coder_agent): Run TRAIN_EVAL -> MODEL_SELECT -> REPORT to create metrics + model card + report.
Then STOP.

If present:
- Do not invent findings. Every result must come from the saved artifacts + mm.doc_search snippets.
- Ensure WORK_DIR/report.md exists.

OUTPUT:
Either full report text OR the single MISSING_OUTPUTS delegation above.

{MEMORY_INSTRUCTIONS}
""".strip()

planner_agent = ConversableAgent("planner_agent", system_message=planner_message, llm_config=llm_config)
pavement_engineer_agent = ConversableAgent("pavement_engineer_agent", system_message=pavement_engineer_message, llm_config=llm_config)
data_scientist_agent = ConversableAgent("data_scientist_agent", system_message=data_scientist_message, llm_config=llm_config)

coder_agent = ConversableAgent(
    "coder_agent",
    system_message=coder_message,
    llm_config=llm_config,
    code_execution_config={"work_dir": WORK_DIR, "use_docker": False, "timeout": 900, "last_n_messages": 30},
)

reviewer_agent = ConversableAgent("reviewer_agent", system_message=reviewer_message, llm_config=llm_config)
librarian_agent = ConversableAgent("librarian_agent", system_message=librarian_message, llm_config=llm_config)
researcher_agent = ConversableAgent("researcher_agent", system_message=researcher_message, llm_config=llm_config)

groupchat = GroupChat(
    agents=[planner_agent, coder_agent, pavement_engineer_agent, data_scientist_agent, reviewer_agent, librarian_agent, researcher_agent],
    messages=[],
    speaker_selection_method="auto",
    max_round=150,
)

def is_termination_msg(msg):
    if not msg:
        return False
    content = msg.get("content", "") if isinstance(msg, dict) else str(msg)
    return ("TERMINATE" in content) or ("PIPELINE_STATE_COMPLETED=STOP" in content)

manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config, is_termination_msg=is_termination_msg)

print("✅ AutoGen initialized.")

"""## 6) Kickoff message (DISCOVER_DATASET → PAPER_INGEST → LOAD_VALIDATE)"""

kickoff_msg = f"""
STATE_NOW: DISCOVER_DATASET

MISSION (NON-NEGOTIABLE):
Autonomously initiate a reproducible pipeline to build a future pavement condition prediction system
using paired baseline → future condition metrics, compatible with HPMS/LTPP-style datasets.

PRIMARY ENGINEERING TARGET RULE:
- Use paired condition columns:
    *_x  → baseline/current condition
    *_y  → future condition (prediction target)
- Strongly prefer IRI / Roughness family:
    e.g., IRI_mean_x → IRI_mean_y
- Do NOT select the final target yet (candidates only).

DATASET PINNING (STRICT):
- Always use the registry (no hardcoded FILE_PATH).
- Load:
    reg = mm.rag_get_latest_registry()
    FILE_PATH = reg["latest_path"]
    DATASET_ID = reg["latest_dataset_id"]
- For this run:
    reg["latest_path"] MUST equal PINNED_FILE_PATH = "{PINNED_FILE_PATH}"
- Do NOT rescan DATASET_DIR or switch datasets unless Planner explicitly disables pinning.

CONFIG CONTEXT (READ-ONLY):
WORK_DIR        = {WORK_DIR}
DATASET_DIR     = {DATASET_DIR}
PAPERS_DIR      = {PAPERS_DIR}
PINNED_FILE_PATH= {PINNED_FILE_PATH}

HARD GATES — DISCOVER_DATASET IS COMPLETE ONLY IF ALL PASS:
1) File exists:
   - WORK_DIR/target_candidates.csv
2) Memory exists:
   - mm record with kind="target_candidates" (meta contains csv_path)
3) Pipeline memory exists:
   - mm record with kind="pipeline_state"
     text="PIPELINE_STATE_COMPLETED=DISCOVER_DATASET"

DELEGATION (to coder_agent ONLY):
Run ONE (and only one) Python block for DISCOVER_DATASET.

MANDATORY EXECUTION STEPS (IN ORDER):
1) from mm_runtime import mm
2) mm.health_check()

3) Anti-loop check:
   - mm.rag_search("PIPELINE_STATE_COMPLETED=DISCOVER_DATASET",
                   kind="pipeline_state", k=5)
   - If found for same dataset_id → do NOT rerun; print end prints and stop.

4) Registry:
   - reg = mm.rag_get_latest_registry()
   - If registry missing:
       - If PINNED_FILE_PATH is available, create a pinned registry where latest_path=PINNED_FILE_PATH.
       - Otherwise build via mm.registry_build(DATASET_DIR).
   - Assert pinning:
       assert reg["latest_path"] == "{PINNED_FILE_PATH}"

5) Load dataframe from FILE_PATH and print:
   - df shape
   - columns list length
   - head(3)

6) Identify condition metric pairs:
   - paired columns ending with *_x and *_y
   - metric families include:
       iri, roughness, pci, rut, crack, fault, skid, friction, condition

7) Build WORK_DIR/target_candidates.csv with EXACT columns:
   - pair_name
   - baseline_col
   - target_col
   - metric_family
   - baseline_coverage   (non-null %)
   - target_coverage     (non-null %)
   - leakage_rule        (must say: "exclude all *_y except target")
   - notes

8) Rank candidates:
   - IRI/Roughness pairs first
   - then highest target_coverage

9) Write memory:
   - mm.rag_add(kind="target_candidates",
                meta={{"csv_path": "{WORK_DIR}/target_candidates.csv",
                      "dataset_id": DATASET_ID,
                      "file_path": FILE_PATH,
                      "recommended_pair": "<best_pair_name>"}})
   - mm.rag_add(kind="pipeline_state",
                text="PIPELINE_STATE_COMPLETED=DISCOVER_DATASET",
                meta={{"dataset_id": DATASET_ID,
                      "file_path": FILE_PATH,
                      "artifacts_saved": ["{WORK_DIR}/target_candidates.csv"],
                      "assumptions": ["pinned registry enforced"]}})

MANDATORY END PRINTS (EXACT — NO EXTRA TEXT):
PIPELINE_STATE_COMPLETED=DISCOVER_DATASET
NEXT_RECOMMENDED_STATE=PAPER_INGEST
ARTIFACTS_SAVED=["{WORK_DIR}/target_candidates.csv"]

STATE_NEXT: PAPER_INGEST
""".strip()


user.initiate_chat(manager, message=kickoff_msg, max_turns=8)

