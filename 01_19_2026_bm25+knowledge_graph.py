# -*- coding: utf-8 -*-
"""01-19-2026-BM25+Knowledge-graph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/167Hs6y5qAEMmv57YK0egEPaq8Aco4cwO
"""

# =========================
# 0) Install (RELIABLE)
# =========================
!pip -q uninstall -y autogen pyautogen ag2 >/dev/null 2>&1
!pip -q install -U "ag2[openai]" python-dotenv

!pip show ag2
!python -c "import autogen; print('autogen version:', getattr(autogen,'__version__', 'unknown'))"

# ============================================================
# 1) Env (DO NOT hardcode keys)
#   Colab: Runtime > Secrets > add OPENAI_API_KEY
# ============================================================
import os, json, time, re, hashlib, sqlite3, sys
from dotenv import load_dotenv
load_dotenv()

# Read from Colab Secrets / .env
os.environ["OPENAI_API_KEY"] =  # set via Secrets
os.environ["OPENAI_MODEL_NAME"] = os.environ.get("OPENAI_MODEL_NAME", "gpt-4o-mini")

print("OPENAI_MODEL_NAME:", os.environ["OPENAI_MODEL_NAME"])
print("OPENAI_API_KEY present?", bool(os.getenv("OPENAI_API_KEY")))

# ============================================================
# 2) Mount Drive + File Path(s)
# ============================================================
from google.colab import drive

FILE_PATH = "/content/drive/MyDrive/Network Maintenace - With & without maintenace/Original-data/2017-2019 (no main).csv"
csv_paths = [FILE_PATH]

if not os.path.exists("/content/drive/MyDrive"):
    drive.mount("/content/drive", force_remount=True)

for p in csv_paths:
    if not os.path.exists(p):
        raise FileNotFoundError(f"CSV path not found:\n{p}")

print("Mounted?", os.path.exists("/content/drive/MyDrive"))
print("CSV paths exist?", all(os.path.exists(p) for p in csv_paths))

# ============================================================
# 3) Workspace + Hybrid Persistent Memory (Notebook version)
#   Then EXPORT to mm_runtime.py (AutoGen-safe)
# ============================================================
from typing import Optional, List, Dict, Any, Tuple
import numpy as np
import pandas as pd
import networkx as nx

WORK_DIR = "/content/pavement_agentic_workspace"
os.makedirs(WORK_DIR, exist_ok=True)

MEM_PATH      = os.path.join(WORK_DIR, "memory.jsonl")
DB_PATH       = os.path.join(WORK_DIR, "memory_hybrid.sqlite")
VEC_PATH      = os.path.join(WORK_DIR, "memory_vectors.npz")
KG_PATH       = os.path.join(WORK_DIR, "knowledge_graph.graphml")
REGISTRY_PATH = os.path.join(WORK_DIR, "file_path_registry.json")

# -------------------------
# JSONL audit
# -------------------------
def _mem_load_raw() -> List[Dict[str, Any]]:
    if not os.path.exists(MEM_PATH):
        return []
    rows = []
    with open(MEM_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rows.append(json.loads(line))
            except Exception:
                pass
    return rows

def mem_add_jsonl(kind: str, text: str, meta: Optional[dict] = None) -> str:
    meta = meta or {}
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    raw = f"{ts}|{kind}|{text}|{json.dumps(meta, sort_keys=True)}".encode("utf-8")
    memory_id = hashlib.sha256(raw).hexdigest()[:16]
    rec = {"id": memory_id, "ts": ts, "kind": str(kind), "text": str(text).strip(), "meta": meta}
    os.makedirs(WORK_DIR, exist_ok=True)
    with open(MEM_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    return memory_id

# -------------------------
# SQLite hybrid store
# -------------------------
def _db_connect() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH)
    conn.execute("PRAGMA journal_mode=WAL;")

    conn.execute(
        "CREATE TABLE IF NOT EXISTS memory_struct ("
        "id TEXT PRIMARY KEY, ts TEXT, kind TEXT, text TEXT, meta_json TEXT"
        ");"
    )
    conn.execute(
        "CREATE VIRTUAL TABLE IF NOT EXISTS memory_fts USING fts5("
        "id UNINDEXED, kind, text, content='', tokenize='porter'"
        ");"
    )
    conn.execute(
        "CREATE TABLE IF NOT EXISTS doc_summaries ("
        "doc_id TEXT PRIMARY KEY, title TEXT, file_path TEXT, summary TEXT, meta_json TEXT"
        ");"
    )
    conn.execute(
        "CREATE TABLE IF NOT EXISTS nodes ("
        "node_id TEXT PRIMARY KEY, node_type TEXT, title TEXT, text TEXT, doc_id TEXT, meta_json TEXT"
        ");"
    )
    conn.execute(
        "CREATE TABLE IF NOT EXISTS edges ("
        "src TEXT, rel TEXT, dst TEXT, meta_json TEXT, PRIMARY KEY (src, rel, dst)"
        ");"
    )

    conn.commit()
    return conn

def keyword_index_add(memory_id: str, ts: str, kind: str, text: str, meta: dict):
    conn = _db_connect()
    conn.execute(
        "INSERT OR REPLACE INTO memory_struct(id, ts, kind, text, meta_json) VALUES (?, ?, ?, ?, ?)",
        (memory_id, ts, kind, text, json.dumps(meta, ensure_ascii=False))
    )
    conn.execute("INSERT INTO memory_fts(id, kind, text) VALUES (?, ?, ?)", (memory_id, kind, text))
    conn.commit()
    conn.close()

def keyword_search(query: str, k: int = 5, kind: Optional[str] = None) -> List[Dict[str, Any]]:
    conn = _db_connect()
    where_kind = "AND kind = ?" if kind else ""
    params = [query]
    if kind:
        params.append(kind)
    params.append(k)

    sql = (
        "SELECT id, kind, text, bm25(memory_fts) AS score "
        "FROM memory_fts "
        "WHERE memory_fts MATCH ? "
        f"{where_kind} "
        "ORDER BY score "
        "LIMIT ?;"
    )
    rows = conn.execute(sql, params).fetchall()
    conn.close()

    out = []
    for rid, rkind, rtext, score in rows:
        out.append({"id": rid, "kind": rkind, "text": rtext, "score": float(-score)})
    return out

# -------------------------
# Semantic vectors (NPZ)
# -------------------------
def _load_vec_store() -> Tuple[List[str], np.ndarray]:
    if not os.path.exists(VEC_PATH):
        return [], np.zeros((0, 0), dtype=np.float32)
    data = np.load(VEC_PATH, allow_pickle=True)
    return data["ids"].tolist(), data["vecs"].astype(np.float32)

def _save_vec_store(ids: List[str], vecs: np.ndarray):
    np.savez(VEC_PATH, ids=np.array(ids, dtype=object), vecs=vecs.astype(np.float32))

def _normalize_rows(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return X
    norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12
    return X / norms

def embed_text(text: str) -> np.ndarray:
    try:
        from sentence_transformers import SentenceTransformer
        global _ST_MODEL
        if "_ST_MODEL" not in globals():
            _ST_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
        v = _ST_MODEL.encode([text], normalize_embeddings=True)[0]
        return np.array(v, dtype=np.float32)
    except Exception:
        dim = 512
        v = np.zeros(dim, dtype=np.float32)
        for token in text.lower().split():
            h = int(hashlib.sha256(token.encode()).hexdigest(), 16)
            v[h % dim] += 1.0
        v = v / (np.linalg.norm(v) + 1e-12)
        return v

def semantic_add(memory_id: str, text: str):
    ids, vecs = _load_vec_store()
    v = embed_text(text).reshape(1, -1)

    if vecs.size == 0:
        ids = [memory_id]
        vecs = v
    else:
        if v.shape[1] != vecs.shape[1]:
            raise ValueError(f"Embedding dim mismatch: got {v.shape[1]} vs store {vecs.shape[1]}")
        ids.append(memory_id)
        vecs = np.vstack([vecs, v])

    _save_vec_store(ids, vecs)

def semantic_search(query: str, k: int = 5, kind: Optional[str] = None) -> List[Dict[str, Any]]:
    ids, vecs = _load_vec_store()
    if vecs.size == 0:
        return []

    q = embed_text(query).reshape(1, -1).astype(np.float32)
    vecs_n = _normalize_rows(vecs)
    q_n = _normalize_rows(q)
    sims = (vecs_n @ q_n.T).ravel()
    top = np.argsort(-sims)[: min(k * 5, len(ids))]

    # kind filter using memory_struct (optional)
    id2kind = None
    if kind and len(top) > 0:
        conn = _db_connect()
        ph = ",".join(["?"] * len(top))
        rows = conn.execute(
            f"SELECT id, kind FROM memory_struct WHERE id IN ({ph})",
            [ids[i] for i in top]
        ).fetchall()
        conn.close()
        id2kind = {rid: rk for rid, rk in rows}

    out = []
    for i in top:
        rid = ids[i]
        if kind and id2kind and (id2kind.get(rid) != kind):
            continue
        out.append({"id": rid, "score": float(sims[i])})
        if len(out) >= k:
            break
    return out

# -------------------------
# KG (NetworkX)
# -------------------------
def _kg_load() -> nx.MultiDiGraph:
    if os.path.exists(KG_PATH):
        try:
            g = nx.read_graphml(KG_PATH)
            mg = nx.MultiDiGraph()
            mg.add_nodes_from(g.nodes(data=True))
            mg.add_edges_from(g.edges(data=True))
            return mg
        except Exception:
            pass
    return nx.MultiDiGraph()

_KG = _kg_load()

def _kg_save():
    g = nx.DiGraph()
    g.add_nodes_from(_KG.nodes(data=True))
    for u, v, data in _KG.edges(data=True):
        g.add_edge(u, v, **data)
    nx.write_graphml(g, KG_PATH)

def kg_add_fact(subj: str, pred: str, obj: str, confidence: float = 1.0, meta: Optional[dict] = None, index_to_memory: bool = True):
    meta = meta or {}
    subj = str(subj); pred = str(pred); obj = str(obj)

    _KG.add_node(subj)
    _KG.add_node(obj)
    _KG.add_edge(subj, obj, relation=pred, confidence=float(confidence), **meta)
    _kg_save()

    if index_to_memory:
        mem_add_jsonl("kg_fact", f"KG FACT: ({subj}) -[{pred}]-> ({obj}) conf={confidence}",
                      {"subj": subj, "pred": pred, "obj": obj, "confidence": float(confidence), **meta})

# -------------------------
# Registry + artifacts
# -------------------------
def _stable_dataset_id(file_path: str) -> str:
    st = os.stat(file_path)
    raw = f"{file_path}|{st.st_size}|{int(st.st_mtime)}".encode("utf-8")
    return hashlib.sha256(raw).hexdigest()[:12]

def registry_build_from_csv_paths(csv_paths: List[str]) -> Dict[str, Any]:
    mapping = {p: _stable_dataset_id(p) for p in csv_paths}
    reg = {
        "created_ts": time.strftime("%Y-%m-%d %H:%M:%S"),
        "path_to_dataset_id": mapping,
        "latest_path": list(mapping.keys())[-1],
        "latest_dataset_id": list(mapping.values())[-1],
    }
    with open(REGISTRY_PATH, "w", encoding="utf-8") as f:
        json.dump(reg, f, ensure_ascii=False, indent=2)
    return reg

def registry_load() -> Optional[Dict[str, Any]]:
    if not os.path.exists(REGISTRY_PATH):
        return None
    try:
        with open(REGISTRY_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def store_dataset_artifacts(file_path: str, dataset_id: str):
    df = pd.read_csv(file_path)
    artifacts = {
        "dataset_id": dataset_id,
        "file_path": file_path,
        "shape": list(df.shape),
        "columns": list(df.columns),
        "dtypes": {c: str(df[c].dtype) for c in df.columns},
        "head": df.head(5).to_dict(orient="records"),
    }
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    text = f"DATASET COLUMNS {dataset_id}: " + ", ".join(list(df.columns)[:80])
    mid = mem_add_jsonl("dataset_artifacts", text, {"dataset_id": dataset_id, "file_path": file_path, "artifacts": artifacts})
    keyword_index_add(mid, ts, "dataset_artifacts", text, {"dataset_id": dataset_id, "file_path": file_path})
    try:
        semantic_add(mid, text)
    except Exception:
        pass

    kg_add_fact(f"dataset:{dataset_id}", "file_path", file_path, confidence=1.0, meta={}, index_to_memory=False)
    for c in df.columns:
        kg_add_fact(f"dataset:{dataset_id}", "has_column", c, confidence=1.0, meta={}, index_to_memory=False)

# -------------------------
# mm wrapper (Notebook)
# -------------------------
class MM:
    def health_check(self) -> bool:
        os.makedirs(WORK_DIR, exist_ok=True)
        if not os.path.exists(MEM_PATH):
            open(MEM_PATH, "a", encoding="utf-8").close()
        conn = _db_connect()
        conn.close()
        return True

    def rag_add(self, kind: str, text: str, meta: Optional[dict] = None):
        meta = meta or {}
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        mid = mem_add_jsonl(kind, text, meta)
        keyword_index_add(mid, ts, kind, text, meta)
        try:
            semantic_add(mid, text)
        except Exception:
            pass
        return True

    def rag_search(self, query: str, k: int = 5, kind: Optional[str] = None, alpha: float = 0.65):
        kw = keyword_search(query, k=k*2, kind=kind)
        sem = semantic_search(query, k=k*2, kind=kind)

        scores = {}
        for r in kw:
            scores.setdefault(r["id"], {"kw": 0.0, "sem": 0.0})
            scores[r["id"]]["kw"] = max(scores[r["id"]]["kw"], r["score"])
        for r in sem:
            scores.setdefault(r["id"], {"kw": 0.0, "sem": 0.0})
            scores[r["id"]]["sem"] = max(scores[r["id"]]["sem"], r["score"])

        if not scores:
            return []

        kw_vals = np.array([v["kw"] for v in scores.values()], dtype=np.float32)
        sem_vals = np.array([v["sem"] for v in scores.values()], dtype=np.float32)

        kw_norm = (kw_vals - kw_vals.min()) / (kw_vals.max() - kw_vals.min() + 1e-12) if kw_vals.size else kw_vals
        sem_norm = (sem_vals - sem_vals.min()) / (sem_vals.max() - sem_vals.min() + 1e-12) if sem_vals.size else sem_vals

        ids = list(scores.keys())
        for i, rid in enumerate(ids):
            scores[rid]["hybrid"] = float(alpha * kw_norm[i] + (1 - alpha) * sem_norm[i])

        ranked = sorted(ids, key=lambda rid: scores[rid]["hybrid"], reverse=True)[:k]
        all_rows = _mem_load_raw()
        id_to_row = {r.get("id"): r for r in all_rows}

        return [(scores[rid]["hybrid"], id_to_row.get(rid, {"id": rid, "kind": kind, "text": ""})) for rid in ranked]

    def rag_get_latest_registry(self) -> Dict[str, Any]:
        reg = registry_load()
        if reg is None:
            raise RuntimeError(f"Registry not found at {REGISTRY_PATH}. Run registry_build_from_csv_paths(csv_paths).")
        return reg

    def kg_add_fact(self, subj: str, pred: str, obj: str, confidence: float = 1.0, meta: Optional[dict] = None, index_to_memory: bool = True):
        kg_add_fact(subj, pred, obj, confidence=confidence, meta=meta, index_to_memory=index_to_memory)
        return True

mm = MM()
print("mm.health_check():", mm.health_check())
print("WORK_DIR:", WORK_DIR)

# ============================================================
# 3B) Export mm_runtime.py (AutoGen-safe)
#   IMPORTANT: no triple quotes inside this module string.
# ============================================================
import importlib

MM_MODULE_PATH = os.path.join(WORK_DIR, "mm_runtime.py")

mm_runtime_code = r'''
# Auto-generated hybrid memory module for AutoGen executor
# Usage:
#   from mm_runtime import mm

import os, json, time, hashlib, sqlite3
from typing import Optional, List, Dict, Any, Tuple
import numpy as np
import networkx as nx

WORK_DIR = r"__WORK_DIR__"
MEM_PATH      = os.path.join(WORK_DIR, "memory.jsonl")
DB_PATH       = os.path.join(WORK_DIR, "memory_hybrid.sqlite")
VEC_PATH      = os.path.join(WORK_DIR, "memory_vectors.npz")
KG_PATH       = os.path.join(WORK_DIR, "knowledge_graph.graphml")
REGISTRY_PATH = os.path.join(WORK_DIR, "file_path_registry.json")

def _mem_load_raw() -> List[Dict[str, Any]]:
    if not os.path.exists(MEM_PATH):
        return []
    rows = []
    with open(MEM_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rows.append(json.loads(line))
            except Exception:
                pass
    return rows

def mem_add_jsonl(kind: str, text: str, meta: Optional[dict] = None) -> str:
    meta = meta or {}
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    raw = f"{ts}|{kind}|{text}|{json.dumps(meta, sort_keys=True)}".encode("utf-8")
    memory_id = hashlib.sha256(raw).hexdigest()[:16]
    rec = {"id": memory_id, "ts": ts, "kind": str(kind), "text": str(text).strip(), "meta": meta}
    os.makedirs(WORK_DIR, exist_ok=True)
    if not os.path.exists(MEM_PATH):
        open(MEM_PATH, "a", encoding="utf-8").close()
    with open(MEM_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    return memory_id

def _db_connect() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH)
    conn.execute("PRAGMA journal_mode=WAL;")

    conn.execute("CREATE TABLE IF NOT EXISTS memory_struct (id TEXT PRIMARY KEY, ts TEXT, kind TEXT, text TEXT, meta_json TEXT);")
    conn.execute("CREATE VIRTUAL TABLE IF NOT EXISTS memory_fts USING fts5(id UNINDEXED, kind, text, content='', tokenize='porter');")
    conn.execute("CREATE TABLE IF NOT EXISTS doc_summaries (doc_id TEXT PRIMARY KEY, title TEXT, file_path TEXT, summary TEXT, meta_json TEXT);")
    conn.execute("CREATE TABLE IF NOT EXISTS nodes (node_id TEXT PRIMARY KEY, node_type TEXT, title TEXT, text TEXT, doc_id TEXT, meta_json TEXT);")
    conn.execute("CREATE TABLE IF NOT EXISTS edges (src TEXT, rel TEXT, dst TEXT, meta_json TEXT, PRIMARY KEY (src, rel, dst));")

    conn.commit()
    return conn

def keyword_index_add(memory_id: str, ts: str, kind: str, text: str, meta: dict):
    conn = _db_connect()
    conn.execute(
        "INSERT OR REPLACE INTO memory_struct(id, ts, kind, text, meta_json) VALUES (?, ?, ?, ?, ?)",
        (memory_id, ts, kind, text, json.dumps(meta, ensure_ascii=False))
    )
    conn.execute("INSERT INTO memory_fts(id, kind, text) VALUES (?, ?, ?)", (memory_id, kind, text))
    conn.commit()
    conn.close()

def keyword_search(query: str, k: int = 5, kind: Optional[str] = None) -> List[Dict[str, Any]]:
    conn = _db_connect()
    where_kind = " AND kind = ? " if kind else " "
    params = [query]
    if kind:
        params.append(kind)
    params.append(k)

    sql = (
        "SELECT id, kind, text, bm25(memory_fts) AS score "
        "FROM memory_fts "
        "WHERE memory_fts MATCH ? "
        + where_kind +
        "ORDER BY score "
        "LIMIT ?;"
    )
    rows = conn.execute(sql, params).fetchall()
    conn.close()
    return [{"id": rid, "kind": rkind, "text": rtext, "score": float(-score)} for rid, rkind, rtext, score in rows]

def _load_vec_store() -> Tuple[List[str], np.ndarray]:
    if not os.path.exists(VEC_PATH):
        return [], np.zeros((0, 0), dtype=np.float32)
    data = np.load(VEC_PATH, allow_pickle=True)
    return data["ids"].tolist(), data["vecs"].astype(np.float32)

def _save_vec_store(ids: List[str], vecs: np.ndarray):
    np.savez(VEC_PATH, ids=np.array(ids, dtype=object), vecs=vecs.astype(np.float32))

def _normalize_rows(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return X
    norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12
    return X / norms

def embed_text(text: str) -> np.ndarray:
    try:
        from sentence_transformers import SentenceTransformer
        global _ST_MODEL
        if "_ST_MODEL" not in globals():
            _ST_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
        v = _ST_MODEL.encode([text], normalize_embeddings=True)[0]
        return np.array(v, dtype=np.float32)
    except Exception:
        dim = 512
        v = np.zeros(dim, dtype=np.float32)
        for token in text.lower().split():
            h = int(hashlib.sha256(token.encode()).hexdigest(), 16)
            v[h % dim] += 1.0
        v = v / (np.linalg.norm(v) + 1e-12)
        return v

def semantic_add(memory_id: str, text: str):
    ids, vecs = _load_vec_store()
    v = embed_text(text).reshape(1, -1)
    if vecs.size == 0:
        ids = [memory_id]
        vecs = v
    else:
        if v.shape[1] != vecs.shape[1]:
            raise ValueError(f"Embedding dim mismatch: got {v.shape[1]} vs store {vecs.shape[1]}")
        ids.append(memory_id)
        vecs = np.vstack([vecs, v])
    _save_vec_store(ids, vecs)

def semantic_search(query: str, k: int = 5, kind: Optional[str] = None) -> List[Dict[str, Any]]:
    ids, vecs = _load_vec_store()
    if vecs.size == 0:
        return []
    q = embed_text(query).reshape(1, -1).astype(np.float32)
    vecs_n = _normalize_rows(vecs)
    q_n = _normalize_rows(q)
    sims = (vecs_n @ q_n.T).ravel()
    top = np.argsort(-sims)[: min(k * 5, len(ids))]

    id2kind = None
    if kind and len(top) > 0:
        conn = _db_connect()
        ph = ",".join(["?"] * len(top))
        rows = conn.execute(f"SELECT id, kind FROM memory_struct WHERE id IN ({ph})", [ids[i] for i in top]).fetchall()
        conn.close()
        id2kind = {rid: rk for rid, rk in rows}

    out = []
    for i in top:
        rid = ids[i]
        if kind and id2kind and (id2kind.get(rid) != kind):
            continue
        out.append({"id": rid, "score": float(sims[i])})
        if len(out) >= k:
            break
    return out

def registry_load() -> Optional[Dict[str, Any]]:
    if not os.path.exists(REGISTRY_PATH):
        return None
    try:
        with open(REGISTRY_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def health_check() -> bool:
    os.makedirs(WORK_DIR, exist_ok=True)
    if not os.path.exists(MEM_PATH):
        open(MEM_PATH, "a", encoding="utf-8").close()
    conn = _db_connect()
    conn.close()
    return True

class MM:
    def health_check(self) -> bool:
        return health_check()

    def rag_add(self, kind: str, text: str, meta: Optional[dict] = None):
        meta = meta or {}
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        mid = mem_add_jsonl(kind, text, meta)
        keyword_index_add(mid, ts, kind, text, meta)
        try:
            semantic_add(mid, text)
        except Exception:
            pass
        return True

    def rag_search(self, query: str, k: int = 5, kind: Optional[str] = None, alpha: float = 0.65):
        kw = keyword_search(query, k=k*2, kind=kind)
        sem = semantic_search(query, k=k*2, kind=kind)

        scores = {}
        for r in kw:
            scores.setdefault(r["id"], {"kw": 0.0, "sem": 0.0})
            scores[r["id"]]["kw"] = max(scores[r["id"]]["kw"], r["score"])
        for r in sem:
            scores.setdefault(r["id"], {"kw": 0.0, "sem": 0.0})
            scores[r["id"]]["sem"] = max(scores[r["id"]]["sem"], r["score"])

        if not scores:
            return []

        kw_vals = np.array([v["kw"] for v in scores.values()], dtype=np.float32)
        sem_vals = np.array([v["sem"] for v in scores.values()], dtype=np.float32)

        kw_norm = (kw_vals - kw_vals.min()) / (kw_vals.max() - kw_vals.min() + 1e-12) if kw_vals.size else kw_vals
        sem_norm = (sem_vals - sem_vals.min()) / (sem_vals.max() - sem_vals.min() + 1e-12) if sem_vals.size else sem_vals

        ids = list(scores.keys())
        for i, rid in enumerate(ids):
            scores[rid]["hybrid"] = float(alpha * kw_norm[i] + (1 - alpha) * sem_norm[i])

        ranked = sorted(ids, key=lambda rid: scores[rid]["hybrid"], reverse=True)[:k]
        all_rows = _mem_load_raw()
        id_to_row = {r.get("id"): r for r in all_rows}
        return [(scores[rid]["hybrid"], id_to_row.get(rid, {"id": rid, "kind": kind, "text": ""})) for rid in ranked]

    def rag_get_latest_registry(self) -> Dict[str, Any]:
        reg = registry_load()
        if reg is None:
            raise RuntimeError(f"Registry not found at {REGISTRY_PATH}. Build it in the notebook first.")
        return reg

mm = MM()
'''

mm_runtime_code = mm_runtime_code.replace("__WORK_DIR__", WORK_DIR)

with open(MM_MODULE_PATH, "w", encoding="utf-8") as f:
    f.write(mm_runtime_code)

print("✅ mm_runtime.py written to:", MM_MODULE_PATH)

if WORK_DIR not in sys.path:
    sys.path.append(WORK_DIR)

import mm_runtime
importlib.reload(mm_runtime)
print("✅ mm_runtime.mm.health_check():", mm_runtime.mm.health_check())

# ============================================================
# 4) Build registry + store artifacts
# ============================================================
reg = registry_build_from_csv_paths(csv_paths)
for p, dsid in reg["path_to_dataset_id"].items():
    store_dataset_artifacts(p, dsid)

mm.rag_add("registry", f"Registry built with {len(reg['path_to_dataset_id'])} datasets.", reg)
mm.rag_add("run_start", f"Run started. CSV files: {csv_paths}", {"csvs": csv_paths})

print("Latest dataset:", reg["latest_dataset_id"])
print("Latest path:", reg["latest_path"])

# ============================================================
# 5) AutoGen / ag2 setup (YOUR agent lineup)
# ============================================================
from autogen import ConversableAgent, UserProxyAgent, GroupChat, GroupChatManager

llm_config = {
    "config_list": [{"model": os.environ["OPENAI_MODEL_NAME"], "api_key": os.environ["OPENAI_API_KEY"]}],
    "temperature": 0.2,
}

user = UserProxyAgent(
    name="user",
    human_input_mode="NEVER",
    default_auto_reply="NEXT",
    max_consecutive_auto_reply=3,
    code_execution_config={"work_dir": WORK_DIR, "use_docker": False, "timeout": 900, "last_n_messages": 12},
)

# ---------------------------
# Persistent workspace paths
# ---------------------------
WORK_DIR = "/content/pavement_agentic_workspace"
os.makedirs(WORK_DIR, exist_ok=True)

MEM_PATH = os.path.join(WORK_DIR, "memory.jsonl")
REGISTRY_PATH = os.path.join(WORK_DIR, "file_path_registry.json")
KG_PATH = os.path.join(WORK_DIR, "knowledge_graph.graphml")

# ---------------------------
# MEMORY INSTRUCTIONS (for all agents)
# ---------------------------
MEMORY_INSTRUCTIONS = f"""
You have access to a memory manager `mm` (preferred). Use memory to prevent repeating the same step.

PRIORITY ORDER (single source of truth):
1) Use `mm` if available:
   - mm.health_check()
   - mm.rag_add(kind, text, meta)
   - mm.rag_search(query, k)
   - mm.rag_get_latest_registry()
   - mm.kg_add_fact(subj, pred, obj, confidence, meta)

2) If `mm` is NOT available, use local fallback files:
   - REGISTRY_PATH: {REGISTRY_PATH}
   - MEM_PATH: {MEM_PATH}

ANTI-LOOP MEMORY RULE:
- Before repeating any stage, search memory for the last completed state:
  - mm.rag_search("PIPELINE_STATE_COMPLETED", k=3)
  If the same state was completed already, DO NOT redo it. Advance to the next state.

WHAT MUST BE STORED AFTER EVERY coder_agent run:
- mm.rag_add(kind="pipeline_state", text="PIPELINE_STATE_COMPLETED=<STATE>", meta={{...}})
- mm.rag_add(kind="dataset_artifact", text="DATASET_ID=... FILE_PATH=... COLUMNS=[...]", meta={{...}})
- mm.rag_add(kind="results", text="metrics + plot paths", meta={{...}})

DATASET PATH RULE:
- FILE_PATH must be discovered via registry (mm registry preferred; else REGISTRY_PATH).
- The hardcoded FILE_PATH shown in the notebook is only a sanity check, not the source of truth.
"""

PIPELINE_STATES = [
  "DISCOVER_DATASET",
  "LOAD_VALIDATE",
  "EDA",
  "SPLIT",
  "TRAIN_TUNE",
  "EVALUATE",
  "VISUALIZE",
  "EXPLAIN",
  "REPORT",
  "STOP"
]

planner_message = f"""
ROLE: Planner/Orchestrator (Agent-0) for an agentic pavement condition analytics pipeline.

MISSION (single source of truth):
1) Predict pavement condition (prefer roughness-based target like IRI; otherwise closest condition metric).
2) Infer MAINTENANCE vs NO-MAINTENANCE using time-ordered changes in condition (e.g., meaningful IRI improvement).
3) If overlay/thickness/treatment indicators exist, infer TREATMENT TYPE using transparent rule-based bins.
4) Produce an auditable decision log, trained model, test metrics, explainability outputs, and a concise report.

WORKFLOW CONTROL:
- You enforce the finite-state workflow: {PIPELINE_STATES}.
- You NEVER execute Python. All executable work is delegated to coder_agent.
- You must keep the team synchronized: every state transition must create artifacts in memory via coder_agent.

HARD GATES (do not skip):
- Cannot proceed past DISCOVER_DATASET until FILE_PATH is chosen via registry.
- Cannot proceed past LOAD_VALIDATE until dataset artifacts exist in memory (columns/head/signature).
- Cannot proceed to TRAIN_TUNE until Data Scientist provides MODEL_SPEC + TUNING_SPEC.
- Cannot proceed to REPORT until metrics + plots + model artifact paths exist in memory.

ANTI-LOOP ENFORCEMENT:
- If STATE repeats twice OR observations repeat twice, you MUST advance state and delegate a different task.
- Do not ask for installs or remounts unless a concrete error occurred.

WHAT YOU MUST CHECK IN MEMORY BEFORE NEXT STEP:
- mm.rag_search("PIPELINE_STATE_COMPLETED", k=3)
- mm.rag_search(kind="dataset_artifact", query="DATASET_ID", k=3)
- mm.rag_search(kind="model_spec", query="MODEL_SPEC", k=3) when approaching TRAIN_TUNE
- mm.rag_search(kind="results", query="metrics", k=3) before REPORT

MANDATORY DELIVERABLES TO ENSURE (via coder outputs + memory):
- model.pkl (or equivalent), best_params.json, search_results.csv/cv_results.csv
- test_metrics.json (R2, RMSE, WMAPE)
- maintenance_inference.csv (segment/year + maintenance flag if possible)
- treatment_inference.csv (treatment label if thickness/overlay fields exist)
- explainability outputs (feature importance or SHAP summary + saved plot paths)
- decision_log entries per state

OUTPUT FORMAT (always):
STATE_NOW:
DECISIONS: <bullet list, 3-6 bullets>
DELEGATE: <exact next action for the next agent; if coder needed, specify exact code tasks>
STATE_NEXT:

{MEMORY_INSTRUCTIONS}
"""

pavement_engineer_message = f"""
ROLE: Pavement Engineer (Agent-1).
You provide defensible engineering logic for:
- what constitutes a meaningful condition improvement (maintenance signal),
- how to infer treatment categories if thickness/overlay/surface indicators exist,
- and how to communicate assumptions clearly.

YOU DO NOT WRITE CODE. You give implementable rules.

HARD RULES:
- Do NOT invent columns. You must first retrieve df.columns from memory:
  mm.rag_search(kind="dataset_artifact", query="COLUMNS", k=3)
- If thresholds/rules already exist in memory, reuse them:
  mm.rag_search(kind="domain_rules", query="threshold", k=5)

MAINTENANCE INFERENCE GUIDANCE:
- Prefer time-ordered comparisons: condition(t) vs condition(t-1) for same segment/group.
- Improvement = meaningful decrease in roughness (e.g., IRI drop) beyond noise threshold.
- If no segment ID exists, propose conservative fallback (e.g., year-level aggregates only) and warn.

TREATMENT INFERENCE GUIDANCE (only if supported by columns):
- Detect thickness/overlay fields (keywords: thickness, overlay, lift, surface, rehab, treatment, mill).
- Propose transparent bins (quantiles or domain bins) and label categories (e.g., thin overlay vs thick overlay).
- Store rules in memory so the coder can implement exactly.

OUTPUT FORMAT (must follow exactly):
STATE_NOW:
NEW_RULES: <0-3 bullets only if you add new>
FIELDS_USED: <exact column names you relied on>
ASSUMPTIONS: <1-3 bullets, conservative>
DELEGATE (to coder_agent): <exact mm.rag_add/mm.kg_add_fact requests + how to implement rules>
STATE_NEXT:

IF no new rule is possible due to missing columns, respond exactly:
NO-UPDATE. DELEGATE: <one concrete question for coder_agent>

{MEMORY_INSTRUCTIONS}
"""

data_scientist_message = f"""
ROLE: Data Scientist (Agent-2).
You design the analysis + predictive modeling plan for pavement condition datasets.
You do NOT execute code; coder_agent executes.

PRIMARY OBJECTIVES:
1) Predict pavement condition using a roughness-based index (prefer IRI-like target).
2) Infer MAINTENANCE vs NO-MAINTENANCE from meaningful improvements over time.
3) If overlay/thickness/treatment indicators exist, infer TREATMENT TYPE using rule-based logic
   (thresholds retrieved from memory; else request Pavement Engineer assumptions).

HARD GATES:
- You cannot proceed until LOAD_VALIDATE artifacts exist in memory.
  You must check: mm.rag_search(kind="dataset_artifact", query="COLUMNS", k=3)

LEAKAGE RULES:
- Fit preprocessors ONLY on train.
- If time column exists => time-aware split (train earlier, test later).
- If segment/project id exists => group-aware split to prevent leakage across same entity.

MANDATORY SPECS (you must output explicitly):
- TARGET: exact column name
- SPLIT_STRATEGY: type + key + rationale
- FEATURE_SPEC: include/exclude + preprocessing
- MODEL_SPEC: baselines + candidates + final selection rule
- TUNING_SPEC: method + CV + parameter ranges (when STATE_NOW=TRAIN_TUNE)
- METRICS: R2, RMSE, WMAPE (test-set only)

OUTPUT FORMAT (MUST follow exactly):
STATE_NOW: <one of {PIPELINE_STATES}>
TARGET: <exact column name or TBD>
SPLIT_STRATEGY:
  type: <time_aware | group_aware | standard>
  key: <time column | group column | NONE>
  rationale: <1-2 lines>
FEATURE_SPEC:
  include: [<columns or patterns>]
  exclude: [<columns>]
  preprocessing: <what + fit-on-train rule>
MODEL_SPEC:
  baselines: [<models>]
  candidates: [<models>]
  final_choice_rule: <1 line>
TUNING_SPEC: <required when TRAIN_TUNE else 'DEFERRED'>
METRICS: [R2, RMSE, WMAPE]
DELEGATE (to coder_agent): <3-8 bullet tasks with saved artifacts + mm.rag_add requirements>
STATE_NEXT:

{MEMORY_INSTRUCTIONS}
"""

coder_message = f"""
ROLE: Coder Agent (Executes Python).
You implement the pipeline exactly as specified by the Planner and Data Scientist,
and you incorporate Reviewer feedback. You execute ONE self-contained Python code block per step.

========================
ABSOLUTE ANTI-LOOP RULES
========================
- Do NOT run pip installs unless an ImportError occurs.
- Do NOT remount Google Drive if already mounted.
- Do NOT redo LOAD_VALIDATE if it is already completed in memory,
  unless the Planner explicitly asks to re-validate.
- If a code block fails, the next attempt MUST remove or replace the failing line(s).
  Never repeat the same failing call twice.

========================
DATA DISCOVERY (MANDATORY)
========================
You MUST determine FILE_PATH via registry only (never hardcode):
1) Prefer: reg = mm.rag_get_latest_registry()
2) Else: read REGISTRY_PATH
3) Else: build registry from csv_paths ONCE and save to REGISTRY_PATH

You MUST PRINT clearly:
- All datasets in registry (dataset_id → file_path)
- The SELECTED dataset_id
- The SELECTED FILE_PATH

========================
CRITICAL STORAGE RULES
========================
- NEVER store large DataFrames in mm.rag_add text.
- Save large outputs to CSV in WORK_DIR.
- In memory, store only:
  - file paths
  - small summaries (head rows only)
- NEVER use DataFrame.to_json() for large tables.
  Instead use:
    json.dumps(df.head(N).to_dict(orient="records"), default=str)

========================
MANDATORY END-OF-BLOCK PRINTS (ALWAYS)
========================
You MUST print these as plain text:
- PIPELINE_STATE_COMPLETED=<STATE>
- NEXT_RECOMMENDED_STATE=<STATE>   (must be EXACTLY one of {PIPELINE_STATES})
- ARTIFACTS_SAVED=<list of file paths or 'none'>

========================
LOAD_VALIDATE REQUIREMENTS
========================
When performing LOAD_VALIDATE you MUST:
- Load CSV from FILE_PATH (selected via registry)
- Print:
  - SELECTED_DATASET_ID
  - SELECTED_FILE_PATH
  - df.shape
  - df.columns
  - df.head(5)
- Compute a stable columns signature/hash
- Store dataset artifact via:
  mm.rag_add(kind="dataset_artifact", ...)

========================
TARGET DISCOVERY (PAIR-AWARE, GENERAL)
========================
Goal:
- Select a FUTURE/LABEL condition column as TARGET
- Identify the corresponding PRESENT/BASELINE column (if exists)
- Support maintenance vs no-maintenance inference later

A) Candidate detection:
- Identify condition-related columns using keywords:
  ["iri", "rough", "rut", "crack", "pci", "condition", "index"]

B) FUTURE vs PRESENT detection (DO NOT hardcode names):
Priority for FUTURE/LABEL:
1) Suffix "_y" paired with same stem "_x"
2) Tokens: ["future","next","lead","label","target","y"]
3) Year-based columns: choose largest year

Priority for PRESENT/BASELINE:
1) Suffix "_x"
2) Tokens: ["current","present","base","prev","t0","x"]
3) Earlier year (if year-based)

C) Build ranked target table with:
- column
- non_null_percentage
- dtype
- std (numeric only)
- paired_present_column (if detected)
- pair_rule_used

D) TARGET selection rules:
- Prefer FUTURE roughness column if a valid PRESENT pair exists
- Must be numeric
- Prefer IRI-like targets first, then PCI/condition, then others

E) Save + store:
- Save full table to WORK_DIR/target_candidates.csv
- Store via:
  mm.rag_add(
    kind="target_candidates",
    text="TARGET DISCOVERY RESULTS",
    meta={{
      "dataset_id": DATASET_ID,
      "file_path": FILE_PATH,
      "selected_target": TARGET,
      "present_feature": PRESENT_FEATURE,
      "pair_rule_used": "...",
      "summary": <json-serializable head rows>
    }}
  )

You MUST PRINT:
- SELECTED_TARGET
- PRESENT_FEATURE_FOR_DELTA (or NONE)
- PAIR_RULE_USED

========================
MAINTENANCE / NO-MAINTENANCE RULE (STORE EARLY)
========================
If a PRESENT/FUTURE roughness pair exists (IRI-like):
- Define: delta = future - present
- MAINTENANCE if delta < 0   (improvement; roughness decreased)
- NO_MAINTENANCE if delta > 0 (deterioration; roughness increased)
- If |delta| < epsilon → UNKNOWN (if threshold provided by Pavement Engineer)

You MUST store this rule via:
mm.rag_add(kind="domain_rules", text="MAINTENANCE RULE: delta=future-present", meta={...})

========================
TRAIN_TUNE REQUIREMENTS (ONLY WHEN REQUESTED)
========================
When training is requested you MUST:
- Print:
  MODELS_TRIED=[...]
  BEST_MODEL=<name>
  BEST_PARAMS=<json>
- Save to WORK_DIR:
  - model.pkl
  - best_params.json
  - search_results.csv (cv_results)
  - test_metrics.json (R2, RMSE, WMAPE on TEST ONLY)
- Store to memory:
  - mm.rag_add(kind="model_spec", ...)
  - mm.rag_add(kind="tuning_results", ...)
  - mm.rag_add(kind="results", ...)

========================
MAINTENANCE / TREATMENT INFERENCE (POST-TRAIN)
========================
If columns support it:
- Maintenance inference:
  - Apply delta rule using time ordering or paired columns
  - Save maintenance_inference.csv
- Treatment inference:
  - If overlay/thickness fields exist:
    apply Pavement Engineer bins
  - Save treatment_inference.csv

Store all paths via mm.rag_add(kind="results", ...)

========================
EXPLAINABILITY REQUIREMENTS
========================
- Minimum: model-native or permutation importance
- If feasible: SHAP summary plot (post-training only)
- Save plots to WORK_DIR
- Store plot paths via mm.rag_add(kind="results", ...)

========================
FINAL REMINDER
========================
- Never guess column names.
- Never store raw DataFrames in memory.
- Always print what dataset, target, and rules were used.

{MEMORY_INSTRUCTIONS}
"""

reviewer_message = f"""
ROLE: Reviewer Agent (Audit + QA + Stop/Iterate).
You critically audit correctness, leakage, reproducibility, and required deliverables.

CHECKLIST (must verify):
- FILE_PATH chosen via registry (not hardcoded).
- LOAD_VALIDATE completed: df.shape/columns/head + signature stored.
- Check the if the target selection appropriate to predicting future pavement condition using roughness index
- Split prevents leakage (time-aware or group-aware when needed).
- Preprocessing fit only on training data.
- Metrics computed on TEST only: R2, RMSE, WMAPE.
- TRAIN_TUNE outputs exist (when training happens):
  model.pkl, best_params.json, search_results.csv, test_metrics.json
- Maintenance vs no-maintenance inference included when possible.
- Treatment inference included if thickness/overlay fields exist.
- coder prints mandatory lines:
  PIPELINE_STATE_COMPLETED, NEXT_RECOMMENDED_STATE, ARTIFACTS_SAVED

ANTI-LOOP:
- If the planner repeats state twice, instruct planner to advance state.
- If coder repeats identical outputs twice, require a different next-stage code request.

OUTPUT FORMAT (only):
STATUS: <APPROVE or REJECT>
ISSUES: <bullets, line-level and actionable>
FIX_REQUEST: <exact change coder must implement next>
STATE_NEXT: <one of PIPELINE_STATES>

{MEMORY_INSTRUCTIONS}
"""
reporting_message = f"""
ROLE: Reporting Agent (Write final report).
You write a concise technical report ONLY from executed results saved in memory/files.
Do NOT invent results.

HARD GATE:
- If metrics/plots/model artifacts are missing, DO NOT write the report.
  Instead respond with:
  MISSING_OUTPUTS. DELEGATE: <exact request for coder_agent to store missing artifacts via mm.rag_add>

WHAT YOU MUST RETRIEVE BEFORE WRITING:
- dataset artifacts: mm.rag_search(kind="dataset_artifact", query="DATASET_ID", k=3)
- model spec/tuning: mm.rag_search(kind="model_spec", query="MODEL_SPEC", k=3) and mm.rag_search(kind="tuning_results", query="BEST_PARAMS", k=3)
- results: mm.rag_search(kind="results", query="metrics", k=5)
- maintenance/treatment inference paths if present

REPORT STRUCTURE (use headings):
1) Dataset and Target
2) Split Strategy and Leakage Controls
3) Modeling and Hyperparameter Optimization
4) Test Metrics (R2, RMSE, WMAPE)
5) Explainability (what features mattered; cite plots)
6) Maintenance vs No-Maintenance Inference (method + outputs)
7) Treatment-Type Inference (if thickness/overlay exists)
8) Limitations and Recommendations
9) Decision Log Summary (brief)

CITATION RULE:
- Only cite papers if they exist in memory (kind="paper"/doc summaries).
- If none exist, omit literature review or request ingestion.

OUTPUT:
- Final report text only (no code).

{MEMORY_INSTRUCTIONS}
"""

# ---------------------------
# Instantiate agents
# ---------------------------
planner_agent = ConversableAgent(
    name="planner_agent",
    system_message=planner_message,
    llm_config=llm_config
)

pavement_engineer_agent = ConversableAgent(
    name="pavement_engineer_agent",
    system_message=pavement_engineer_message,
    llm_config=llm_config
)

data_scientist_agent = ConversableAgent(
    name="data_scientist_agent",
    system_message=data_scientist_message,
    llm_config=llm_config
)

coder_agent = ConversableAgent(
    name="coder_agent",
    system_message=coder_message,
    llm_config=llm_config,
    code_execution_config={
        "work_dir": WORK_DIR,
        "use_docker": False,
        "timeout": 900,
        "last_n_messages": 30,
    },
)

reviewer_agent = ConversableAgent(
    name="reviewer_agent",
    system_message=reviewer_message,
    llm_config=llm_config
)

reporting_agent = ConversableAgent(
    name="reporting_agent",
    system_message=reporting_message,
    llm_config=llm_config
)

groupchat = GroupChat(
    agents=[planner_agent, coder_agent, pavement_engineer_agent, data_scientist_agent, reviewer_agent, reporting_agent],
    messages=[],
    speaker_selection_method="round_robin",
    max_round=100,
)

def is_termination_msg(msg):
    if not msg:
        return False
    content = msg.get("content", "") if isinstance(msg, dict) else str(msg)
    return ("TERMINATE" in content) or ("PIPELINE_STATE_COMPLETED=STOP" in content)

manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config, is_termination_msg=is_termination_msg)

print("✅ AutoGen initialized with agents:",
      "user, planner_agent, pavement_engineer_agent, data_scientist_agent, coder_agent, reviewer_agent, reporting_agent")

# ============================================================
# 6) Kickoff
# ============================================================
escape_msg = """
STATE_NOW: LOAD_VALIDATE
OBSERVATION:
- Kickoff required to load registry-selected dataset and perform target discovery.
DECISION: Delegate LOAD_VALIDATE + TARGET DISCOVERY execution to coder_agent.
DELEGATION (to coder_agent):
Run EXACTLY ONE self-contained Python code block.

FIRST LINE MUST BE: from mm_runtime import mm

Then do EXACTLY:
1) assert mm.health_check()
2) reg = mm.rag_get_latest_registry()
   print created_ts/latest_dataset_id/latest_path and list all dataset_id -> path
3) FILE_PATH = reg["latest_path"]; DATASET_ID = reg["latest_dataset_id"]
4) df = pd.read_csv(FILE_PATH); print shape/columns/head(5)
5) TARGET DISCOVERY:
   - find candidate columns using keywords: ["iri","rough","rut","crack","pci","condition","index"]
   - compute non_null_pct, dtype AS STRING, std as float/None
   - save full ranked table to WORK_DIR/target_candidates.csv
   - store mm.rag_add(kind="target_candidates", ...) with JSON-SAFE meta only:
       meta must include only strings/numbers/bools/lists/dicts (no pandas dtypes)
       include: dataset_id, file_path, target_candidates_csv_path, head_summary (df.head(5))
6) mm.rag_add(kind="pipeline_state", text="PIPELINE_STATE_COMPLETED=LOAD_VALIDATE", meta={"dataset_id":..., "file_path":...})

MANDATORY END PRINTS (plain prints):
PIPELINE_STATE_COMPLETED=LOAD_VALIDATE
NEXT_RECOMMENDED_STATE=EDA
ARTIFACTS_SAVED=[<full path to target_candidates.csv>]

End with: NEXT: data_scientist_agent
STATE_NEXT: EDA
STOP_CONDITION: dataset artifacts + target_candidates stored
"""

user.initiate_chat(manager, message=escape_msg, max_turns=6)